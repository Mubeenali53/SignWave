{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d25528fc-453e-4783-8a95-8aa92a64c0e1",
   "metadata": {},
   "source": [
    "## Data Pre-Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d376553-e938-4038-a5a6-c2c16a1138dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68e528b8-8973-4eb2-8577-989e389f617e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "header = ['l1', 'l2', 'l3', 'l4', 'l5', 'r1', 'r2', 'r3', 'r4', 'r5', 'l_ax', 'l_ay', 'l_az', 'l_gx', 'l_gy', 'l_gz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4af00f4a-5efe-4fe9-b8d3-f8be4502ab71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l1</th>\n",
       "      <th>l2</th>\n",
       "      <th>l3</th>\n",
       "      <th>l4</th>\n",
       "      <th>l5</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r4</th>\n",
       "      <th>r5</th>\n",
       "      <th>l_ax</th>\n",
       "      <th>l_ay</th>\n",
       "      <th>l_az</th>\n",
       "      <th>l_gx</th>\n",
       "      <th>l_gy</th>\n",
       "      <th>l_gz</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3008</td>\n",
       "      <td>2923</td>\n",
       "      <td>1661</td>\n",
       "      <td>2841</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.377312</td>\n",
       "      <td>-3.088520</td>\n",
       "      <td>6.890512</td>\n",
       "      <td>-0.085801</td>\n",
       "      <td>-0.017853</td>\n",
       "      <td>-0.046897</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3015</td>\n",
       "      <td>2882</td>\n",
       "      <td>1664</td>\n",
       "      <td>2837</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.566454</td>\n",
       "      <td>-3.023877</td>\n",
       "      <td>6.562507</td>\n",
       "      <td>-0.087400</td>\n",
       "      <td>-0.005596</td>\n",
       "      <td>-0.051960</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3024</td>\n",
       "      <td>2906</td>\n",
       "      <td>1654</td>\n",
       "      <td>2835</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.463503</td>\n",
       "      <td>-3.009511</td>\n",
       "      <td>6.694188</td>\n",
       "      <td>-0.102588</td>\n",
       "      <td>0.006662</td>\n",
       "      <td>-0.049562</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3027</td>\n",
       "      <td>2903</td>\n",
       "      <td>1651</td>\n",
       "      <td>2848</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.760384</td>\n",
       "      <td>-2.664747</td>\n",
       "      <td>6.344635</td>\n",
       "      <td>-0.100723</td>\n",
       "      <td>-0.009593</td>\n",
       "      <td>-0.056224</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3023</td>\n",
       "      <td>2901</td>\n",
       "      <td>1663</td>\n",
       "      <td>2834</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.604761</td>\n",
       "      <td>-2.844312</td>\n",
       "      <td>6.550536</td>\n",
       "      <td>-0.100723</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>-0.031176</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     l1    l2    l3    l4  l5  r1  r2  r3  r4  r5      l_ax      l_ay  \\\n",
       "0  3008  2923  1661  2841   0   0   0   0   0   0  8.377312 -3.088520   \n",
       "1  3015  2882  1664  2837   0   0   0   0   0   0  8.566454 -3.023877   \n",
       "2  3024  2906  1654  2835   0   0   0   0   0   0  8.463503 -3.009511   \n",
       "3  3027  2903  1651  2848   0   0   0   0   0   0  8.760384 -2.664747   \n",
       "4  3023  2901  1663  2834   0   0   0   0   0   0  8.604761 -2.844312   \n",
       "\n",
       "       l_az      l_gx      l_gy      l_gz label  \n",
       "0  6.890512 -0.085801 -0.017853 -0.046897     a  \n",
       "1  6.562507 -0.087400 -0.005596 -0.051960     a  \n",
       "2  6.694188 -0.102588  0.006662 -0.049562     a  \n",
       "3  6.344635 -0.100723 -0.009593 -0.056224     a  \n",
       "4  6.550536 -0.100723  0.001599 -0.031176     a  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./final_v1.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6ddbe1c-1b63-4d0b-97be-ad1e061769cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "x           30\n",
       "i love u    26\n",
       "g           26\n",
       "l           26\n",
       "f           25\n",
       "h           25\n",
       "v           23\n",
       "u           23\n",
       "m           23\n",
       "s           23\n",
       "r           23\n",
       "p           23\n",
       "q           22\n",
       "y           22\n",
       "w           22\n",
       "t           22\n",
       "n           22\n",
       "o           22\n",
       "b           22\n",
       "e           22\n",
       "k           21\n",
       "i           21\n",
       "d           20\n",
       "ch          20\n",
       "c           20\n",
       "a           20\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728419c3-7a5d-4cbb-bc8d-eb0a43a450ca",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6dd7db71-740e-47e3-9d41-a22735007fda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2bd79e07-7efc-48d6-86ec-7d0eeab801ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = df['label']\n",
    "X = df.drop(columns=['label'])\n",
    "\n",
    "X = pd.get_dummies(X)\n",
    "X = X.fillna(X.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7e96f536-590e-4fe1-ba34-168afa553de9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7559286e-fd79-44a9-aba4-80edbba303e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model_v1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model_v1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a9f96e5b-f955-4a35-8ab8-2fc9a495b746",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['b', 'n', 'h', 'i', 'p', 'l', 'u', 'y', 'c', 'h', 'f', 'b', 'p',\n",
       "       'q', 'd', 'o', 'w', 's', 'd', 'k', 't', 'v', 'ch', 'c', 'm',\n",
       "       'i love u', 't', 'k', 'p', 'a', 'y', 'y', 'ch', 'p', 'n', 'e',\n",
       "       'ch', 't', 'g', 'l', 'i', 'n', 'ch', 'x', 'q', 't', 'ch', 'h', 'y',\n",
       "       'q', 'x', 'i love u', 'a', 't', 'd', 'h', 'x', 'a', 'w', 'ch',\n",
       "       'i love u', 'h', 'ch', 'm', 'm', 'q', 'f', 'y', 'x', 'e', 'v', 'o',\n",
       "       'v', 'x', 'ch', 'b', 'e', 'y', 'l', 'l', 'n', 'p', 'c', 'w', 'y',\n",
       "       'm', 'u', 'a', 'p', 'd', 'd', 'i', 'g', 'x', 'ch', 'q', 'w', 'm',\n",
       "       'h', 'f', 'n', 'p', 'g', 'n', 'l', 'h', 'x', 'i', 'y', 'a', 'h',\n",
       "       'h', 'u', 'q', 'i love u', 'k', 'd', 'f', 'r'], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rf_model_v1.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5ac512dc-da25-4965-9efe-6d19cd4e8785",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9748\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2cee86d2-7b4a-4a06-982e-b0127f073094",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           a       1.00      1.00      1.00         5\n",
      "           b       1.00      1.00      1.00         3\n",
      "           c       0.67      1.00      0.80         2\n",
      "          ch       1.00      1.00      1.00         9\n",
      "           d       1.00      1.00      1.00         6\n",
      "           e       1.00      1.00      1.00         3\n",
      "           f       1.00      1.00      1.00         4\n",
      "           g       1.00      1.00      1.00         3\n",
      "           h       1.00      1.00      1.00         9\n",
      "           i       1.00      0.80      0.89         5\n",
      "    i love u       1.00      0.80      0.89         5\n",
      "           k       1.00      1.00      1.00         3\n",
      "           l       0.80      1.00      0.89         4\n",
      "           m       1.00      1.00      1.00         5\n",
      "           n       0.83      1.00      0.91         5\n",
      "           o       1.00      1.00      1.00         2\n",
      "           p       1.00      1.00      1.00         7\n",
      "           q       1.00      1.00      1.00         6\n",
      "           r       1.00      1.00      1.00         1\n",
      "           s       1.00      0.50      0.67         2\n",
      "           t       1.00      1.00      1.00         5\n",
      "           u       1.00      1.00      1.00         3\n",
      "           v       1.00      1.00      1.00         3\n",
      "           w       1.00      1.00      1.00         4\n",
      "           x       1.00      1.00      1.00         7\n",
      "           y       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           0.97       119\n",
      "   macro avg       0.97      0.97      0.96       119\n",
      "weighted avg       0.98      0.97      0.97       119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "08ce3c34-bb4c-495f-84c8-62399808a2eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11841094, 0.13527393, 0.11309313, 0.13263142, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.15212632, 0.1550831 , 0.12961671, 0.02354936, 0.01560881,\n",
       "       0.02460628])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model_v1.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ed92adce-9abd-4e22-8638-5a8a54059712",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['l1', 'l2', 'l3', 'l4', 'l5', 'r1', 'r2', 'r3', 'r4', 'r5', 'l_ax',\n",
       "       'l_ay', 'l_az', 'l_gx', 'l_gy', 'l_gz'], dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model_v1.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "31d9661d-39fe-4847-ae99-b37b3e44ee5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAI1CAYAAAA6iQAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABljElEQVR4nO3deVxU9f7H8fcAAuKC5gIuJJr7viGpKVoUluaOoJZKZnWL1Oia2qJ5W8huGpWmaZntWpamZpq5ZWmiopVp2uKWCmqmuKPw/f3hb+YyMaCMRwfk9Xw85qF8z/ec+Zwv5wznPWfmHJsxxggAAAAAcFm8PF0AAAAAAFwLCFcAAAAAYAHCFQAAAABYgHAFAAAAABYgXAEAAACABQhXAAAAAGABwhUAAAAAWIBwBQAAAAAWIFwBAAAAgAUIV8BlsNlseT46dOjg6RILnUGDBslms2nmzJmeLgUF2EcffaQWLVooICBANptNoaGhni7JEh06dMjxOlKiRAnVr19fjz76qA4dOuTpEnP19NNPF7p9d9euXRd9HR80aJCny/QI++/TZrMpKioqz74NGjRw9L3Sv/+ZM2fKZrPp6aefvuxl2fe3Xbt2XfayADsfTxcAXAsGDhzosr1u3bpXtY5BgwbpnXfe0YoVKwh2hUhoaKh2794tY4ynSykU1q9fr7vuukv+/v667bbbVKZMGZUvX97TZVkqKipKwcHBkqQDBw7o+++/18SJEzV79mytW7dOVapU8XCF15YSJUqod+/eLqfddNNNV7ka68ycOVNxcXEaO3bsZYWRZcuWKS0tTUFBQTmmpaSkaOvWrZdRJXBtIVwBFihM79QChd2CBQuUlZWl1157Tffcc4+ny7kiRo0a5fQGyYEDB3TLLbdo27ZtGjt2rN58803PFXcNKl++PK/juWjWrJk2bdqkjz76SMOHD88x/f3335ckNW/eXCkpKVe5OqDg4WOBAIBC5c8//5Qk1ahRw8OVXD2VKlXS2LFjJUlLlizxcDUoSjp37qwyZcrogw8+yDEtMzNTs2bNUp06dRQWFuaB6oCCh3AFXEXbtm3ToEGDFBISIj8/PwUFBSk2NlY///xzjr5nzpzRW2+9pW7duqlGjRoqXry4ypQpo/bt22vWrFk5+ttsNr3zzjuSpI4dOzp9Z8D+eXL795lWrlzpsj5X313J/vn2HTt2KDY2VkFBQfLy8tK8efPcWjd3ZP9s/OzZsxUWFqaAgABVqVJFjz32mDIyMiRJv//+u/r27auKFSsqICBAHTt21I8//phjedm/H7Ju3TpFRUWpTJkyKl26tG699VZ9//33udayaNEi3XrrrSpbtqz8/f1Vp04djRo1SkePHs3zeZKTk9WlSxeVK1dONptNSUlJstls2r17tyTn7/Bl/z389ttvevrpp9W6dWsFBwfL19dXVatW1YABA7Rjxw6XNdqXkZmZqfHjx6t27dry8/NTSEiIRo4cqbNnz7qc7+TJkxo/frxatmyp0qVLq0SJEqpbt64eeughl8+1bt06RUdHq1KlSo667r33Xu3ZsydHX2OMPvjgA910000KCgqSv7+/QkJCFBkZqcmTJ+c63nb2bfHtt9+W5Lyd2886ZN/GlyxZoo4dO6pMmTKy2WyO38/58+f12muvqUWLFipZsqRKliypVq1aacqUKcrMzMzxvFZve+5q0KCBJOngwYM5pq1evVrx8fFq3LixypYtq+LFi6tu3bq5bpcrV650fJ/oyJEj+te//qVKlSrJz89PDRs21IwZM3KtY/78+WrdurUCAgJUrlw59erVK9ft0G7v3r26//77Va1aNfn5+alixYrq2bOn1q9fn6Ov/XtQHTp00MmTJ5WQkKCQkBAVL15czZs314IFCxx9P/nkE4WHh6tEiRIKCgrS0KFDdfr06TxruVxbt25V//79Hdt8lSpVNGDAAG3fvj1H3+zjnJqaqnvvvVdVq1aVj4+PkpKSHP327t2r+Ph43XDDDfL399d1112nLl26aM2aNS5rWLNmjbp37+4Yz+DgYLVq1UqjRo3SiRMnJF3YbuPi4iRJ48aNc3p9yc9ZOj8/P/Xu3VsbNmzIsY7Lli3TgQMH1L9//zyX8ddff2nEiBGqVauWY/06deqkr776Ktd5vvvuO0VGRqpUqVIqU6aMoqKitG7dujyf5/z585oyZYpat26t0qVLq3jx4mratKmSkpJ0/vz5S15n4LIYAG6TZC51N5o7d67x8/MzkkzTpk1N7969TXh4uLHZbCYgIMCsWrXKqf+2bduMJFO5cmXTsWNHExMTYyIiIkyxYsWMJDN27Fin/gMHDjQ33HCDkWSioqLMwIEDHY9Dhw45+kgyK1asyHV9qlWr5tT29ttvG0kmNjbWlC5d2lSvXt3ExMSY2267zSxcuNCtdcuLvca3337bqT0iIsJIMsOHDzc+Pj4mMjLS9OjRw5QvX95IMgMGDDA7duww5cuXN3Xr1jUxMTGmUaNGRpK57rrrTGpqqtPyxo4daySZIUOGGF9fX1O/fn0TGxtrWrZsaSQZX19fs2TJkhz1Pf/880aS8fHxMbfccouJiYkxVatWNZJM7dq1c32euLg4U6xYMdOgQQMTGxtr2rdvb3744QczcOBAU6JECSPJ6Xf26KOPOpYxcuRIY7PZTKNGjUyXLl1Mr169TL169YwkU7p0afPDDz/k+rvs06ePKVmypOnSpYvp0qWLCQwMNJJM//79c8yzf/9+06BBAyPJlC1b1tx5552md+/epnnz5sbLy8u8/PLLTv0nT55svLy8jJeXlwkPDzfR0dGmcePGRpKpUKGC2bp1q1P/f//730aS8fPzM7feeqvp27ev6dixo6lQoUKO7c6V1atX57qdr1692hjzv+1nyJAhxmazmbCwMBMbG2vCwsLM0aNHzfnz580dd9zhGLvu3bubbt26mVKlShlJpkePHiYzM9Ppea3e9vJify5X++iaNWuMJFO1atUc08LDw42/v79p1aqV6dWrl+ncubOpVKmSkWQaNGhgjh8/7tR/xYoVRpLp1q2bqV27tqlcubKJjo42HTt2NN7e3kaSmT59eo7nmTJlipFkbDabad++vYmJiTHVqlUzgYGB5q677nK57/7444+OsapTp46JjY01bdq0cexHH3/8sVP/nTt3GkmmdevWJjw83FSsWNH07t3bdOjQwXh5eRlvb2+zdOlSM3HiRMd+2KNHD1OuXDkjyfTr1++Sx9v+XJey/RljzNdff22KFy9uJJlmzZqZ2NhY07RpUyPJlCxZ0nzzzTcux/mOO+4wVatWNcHBwaZ3796mS5cu5o033jDGXPi9li1b1jE+PXv2NO3atTM+Pj7G29vbzJo1y2mZ8+fPN15eXsZms5nw8HATGxtrOnXq5Ngvdu7caYwxJjEx0bRt29ZIMk2aNHF6fbHvL3mxv3Y988wzjvV46qmnnPoMGDDASDK///67uf/++13+/v/8809To0YNI8lcf/31JiYmxtx8882O7WzixIk5nnvBggXGx8fHSDKtWrUysbGxpl69esbX19fcd999Lv8Gnjp1ynTs2NGx3916663mzjvvNBUrVjSSTNeuXXPdt+1jBliBcAVchksNVzt37jQlSpQwJUuWNEuXLnWa9uWXX5pixYqZkJAQc/bsWUf74cOHzdKlS01WVpZT/z/++MOEhoYaLy+vHH8QLhaeLidcSTLx8fHm/Pnzl71ueblYuCpZsqRZv369o/3AgQMmKCjI2Gw2U69ePTNq1CjHmGVlZZm7777bSDJjxoxxWp79wEGSeeKJJ5zG+fXXXzeSTKVKlcypU6cc7cnJycbLy8uULFnSfP/99472M2fOmOjoaCPJ9OrVK9fnGT9+vMt1rlatWp7b0dq1a80ff/yRo33GjBlGkunYsWOOafbnrFevnjlw4ICj/Y8//jBlypQxksxvv/3mNM8tt9xiJJk+ffrkOBjfuXOnU4hbu3at8fb2NlWqVDEbNmxw6vvmm28aSSY8PNzRdvr0aePn52dKlSqVY13OnTuX46A0L3ltx/ZpknIclBpjzEsvveQIHNlDz/79+02dOnWMJPPaa685zWP1tpeXvMLVmDFjjCRz77335pi2aNEic/ToUae2M2fOOA5Ex40b5zTNfrBsf+PkzJkzjmlz5851HAhnt2vXLuPv72+KFStmFi9e7GjPyMgw/fv3dywv+76blZXlCJqPPfaY0342Z84cx/60f/9+R7s98EgyN998szlx4oRjmv31qGbNmqZs2bJOv499+/Y5DqR///33HGPkSn7C1YkTJ0xQUJCRZCZNmuQ0beLEiY7ge/r0aUd79nHu0aOH0zRjjDl27JipVKmS8fb2Nu+//77TtPXr15uyZcuakiVLmoMHDzra27dvbySZOXPm5KgxOTnZpKenO362j9c/g8ilyB6usrKyTEhIiKlRo4Zj+qlTp0ypUqVM69atjTEm13DVpUsXR+jN/ndg9erVJiAgwHh7e5tNmzY52tPT002FChWMJDNjxgxHe1ZWlhk5cqRjPP+5Tg8++KCRZGJiYpz2hfT0dMcbKlOmTHGah3CFK4FwBVwG+4t8bg/7C/awYcNcHrTZDR061Egyn3322SU97/Tp040k8+qrrzq1X8lwVaFCBXPy5Mkc81i9bhcLV08++WSOeR555BEjydSoUcNkZGQ4Tfvhhx+MJBMREeHUbj9wqFatmjl37lyOZYaHhxtJ5r333nO02d+lHT16dI7+aWlppnjx4sbLy8vs2bMnx/M0atQoR1C2u1i4ykvbtm2NzWbLcWBt3wb/GXiNMSY+Pj7HGK9bt85IMhUrVnQ6OMtNt27djCSzYMECl9O7du1qJJmUlBRjzIXxkS6c2bxclxKuOnfu7HLe66+/3khyeVZy/vz5jgP37Kze9vLiKlzt37/fvPbaa8bf39/UrFnTKYhczKlTp4yPj49p3ry5U7v9oL906dLm8OHDOeZr2LBhjoNOe7gbMGBAjv6HDx82AQEBObar5cuXO4LaP8fHGGN69uxpJJlnn33W0WYPPF5eXmb79u1O/TMzMx1nwfL6ffzz9SM32YNcbg87+5sZ9jDxTy1atDCSnEKSfZz9/PzMn3/+mWOel19+2UhyOlOdnT20ZT+7Yz9r/c993hWrwpUxxjz22GNGklmzZo0xxpgPP/zQSDKTJ082xrgOV7///rvjjYm//vorx3MkJCTkeMPAPs7t27fP0T8jI8PxSYHs65SWluZ4Iy/7G2J2Bw4cML6+vqZx48ZO7YQrXAlcLRCwQG6XYi9ZsqQkOT5X3rNnT5f92rVrp1dffVXJycnq0aOH07Rvv/1WK1eu1L59+3TmzBkZY3TgwAFJ0q+//mrVKlxUZGSkAgICcrRfzrq547bbbsvRZr+wQYcOHVSsWDGX0+xj9k+9evWSj0/Ol8K+fftq3bp1Wr16te666y5JF77XIsnl9wsqVqyo2267TZ9//rm+++47xcbGOk3v0qWLbDbbxVYvVydOnNCCBQu0efNmHTlyROfOnXOslzFGv//+u5o3b+40T7FixdSxY8ccy6pdu7ZjXruvv/5a0oX1LlWqVJ61ZGVladmyZQoICMj1/jft2rXT/PnzlZycrGbNmqlixYqqWrWqNm/erFGjRum+++67ohek6Nq1a462PXv2aM+ePapQoYLL7ahLly4qU6aMfvvtN6WmpjouhW5n9baXF1e/t+bNm2vFihUqXbq0y3n27dunBQsW6JdfflF6erqysrIkSb6+vrm+VrRo0ULlypXL0V67dm1t2bJFBw4ccHz/z779/3PblqRy5crptttuc/oeZvZ5+vTpk2N8JOnuu+/WZ5995uiXXWhoqGNbtfPy8lK1atV0+PDhPH8f+R3zvC7FbpfX/i9Jd911lzZu3KjVq1fn6NO8eXOXl8+/lNdPSUpOTna0tWjRQtu2bdPdd9+tp556Si1atJCX15X/Cv1dd92lF198Ue+//75at26t999/X8WKFVNMTEyu83z77beSpE6dOum6667LMf3uu+/WxIkTnX7/eW1nxYoVU+/evZ2+ryZd+G7buXPn1KlTJxUvXjzHfMHBwapVq5Z++uknnT592mUfwCqEK8ACF/tysP2CEhe7N83hw4cd/z927Jh69uyp5cuX59r/+PHjl1zj5br++utdtruzbpfD1fPYQ2xe03K7gEO1atVcttsPKPfv3+9os/8/txvW2tv37duXY1pu43cpli9frtjY2DxvIOtqWwgODpa3t3eOdnt4yj4me/fulSTdcMMNF63n8OHDji/N+/r6XrSv3TvvvKPY2FiNHz9e48ePV7Vq1RQREaHY2FjdfvvtF33e/HA13vbfX26/c5vNpmrVquno0aPat29fjnBl9baXF/t9rjIzM7Vz506tWbNGKSkpGjZsmOOCHtlNnDhRo0aNcoTuS1W1alWX7a62kYuNn6v94nL2mdxeU67EmF/KpdivxP5vf/1s27Ztns+dfT96/vnn9dNPP2nBggVasGCBypYtq5tuukldu3Z13P/tSmjUqJEaN26sjz/+WE888YS++uor3X777S7DuZ07Y+bOdmYfx+nTp2v69Ol5rseRI0e4TxyuKMIVcBXY30HO7QyXXXh4uOP/I0eO1PLlyxUREaFx48apYcOGKlOmjLy9vfXVV18pKirK0pvO2mvMTW5/sN1Zt8uR1zu0V+Pd27zkdWbK3QOeEydOqE+fPjpy5IjGjBmj2NhYVatWTcWLF5fNZlO/fv300UcfudwWrtR42H/nJUuWVK9evfLsa7/CnSTdfPPN+u2337Rw4UItXrxYK1eu1Lvvvqt3331XvXr10pw5cyyr0d3xzut3eDW3vX/e5+qbb75RVFSUZs6cqc6dOzudZfn+++/16KOPKjAwUK+88oo6dOig4OBg+fn5SZIqV66c65mcgrzPXKw2T9f+T+7s//Z9qXfv3ipRokSu82e/IX1ISIg2bNig5cuXa+HChVq1apUjaL344otau3ZtnoHncvTv318jR47U4MGDdf78ecdZfXddztn87Ozj2LRpUzVp0iTPvvb9ArhSCFfAVVC1alX9/vvvmjBhwiX/0Zs7d668vb01f/78HB8D+uOPP9yqw36WwX7WITv7mYv8cmfdChL7ZdBza69cubKjrXLlytq5c6d2796t+vXr55jnUs/i5cfq1av1119/qXfv3ho3blyO6e5uC/8UEhIi6cLlxC+mfPny8vf3l5eXl95+++18HSCVLl1a/fr1U79+/SRdCAbR0dH69NNPtWjRIt1xxx3urcAlsP8uc/udZ59W0N7Zbt++vcaMGaPHH39cjz/+uHr06OE4Kzl37lxJ0nPPPZfjTY7Tp08rNTXVkhoqVaqk7du357r9uxrXi435ldhnrpQrsS5Vq1bV9u3bNWrUKLVo0eKS5/Px8dFtt93m+Gjk7t27dc8992j58uUaP368XnzxxUteVn7069dPo0aN0uLFi1W6dGmXH7/Nzp0xq1SpUp7zuGq3n4G96aab9Nprr+W9EsAVVrDe9gGuUbfeequk/x0EXYq///5bpUuXdvn9io8//tjlPPbwlNv9POx/tFzdk2bp0qWXXFt27qxbQfLZZ5+5vLeR/V5iN910k6PN/v2Hjz76KEf/Q4cOacmSJbLZbBf9iM8/5fV7+/vvvyW5/vjWb7/9ppSUlHw9V24iIyMlXVg3V+E7Ox8fH3Xo0EHp6elatmzZZT3vjTfeqLvvvluStGXLlsta1sVcf/31uv7663Xo0CGXdX/xxRf6+++/VbNmzRwfCSwIhg8fruDgYP3666+aPXu2oz2vbeSTTz6x7Ay3fft39fpz5MgRl/csss/zySefuNzP3n//fad+BVle+7/k3rpY9fpZrVo1jRw5UpLzfnSxvwn5VbVqVXXu3FnlypW7pI8g2l8/Fy9e7PJ+a67GLK/t7Pz58/r0009ztHfs2FHe3t5auHBhvj8aC1iNcAVcBY8++qiKFy+uf//73/rss89yTD979qzmzJmjP//809FWu3Zt/f33304HUZL08ssva8WKFS6fx/4uoaubWUpSRESEJGnKlCn666+/HO2bN2/WmDFj8rdS/8+ddStIdu3aleOM0LRp07R27VoFBQU5feztoYcekpeXl1599VVt2LDB0Z6RkaGHH35Yp0+fVs+ePR1ngS5VXr83+xf6P/vsM6fvXB09elSDBw+27ECiVatW6tixow4ePKj77rtPJ0+edJq+a9cu/fTTT46fn3jiCXl5eSkuLs7lTalPnDihGTNmOG7oumfPHs2cOVOnTp1y6nfmzBnH9pzfcXPHww8/LElKSEhwGs/U1FSNGDFCkjRs2LArXoc7ihcvrlGjRkmSEhMTHaHJvo289dZbTtvD1q1bHQfcVoiLi5Ofn58++OADxwVQJOncuXN65JFHcmwz0oULfTRq1Ei7du3SmDFjnILe3Llz9dlnn6lkyZK65557LKvzSunTp4+CgoL07bffatq0aU7T7K8JVapUuehHZbO7//77VbFiRb344ouaNm1ajo9nnz9/XkuWLHEKTC+//LLLs5GLFi2S5LwfXexvgjsWLFigw4cPX9KNv2vUqKHOnTvr+PHjGjZsmNP2uXbtWk2ZMkXe3t566KGHHO3R0dEqV66cVq5cqXfeecfRbozR2LFjXd6gvEqVKrrnnnu0a9cu9e3bV2lpaTn6/Pbbby6DGWA5z12oECj8pEu/ifC8efMclyquWbOmufPOO01sbKxp166d4yay2e/18f777zuW365dO9O3b19Tv3594+Xl5bjc8MCBA52eY8OGDcZmsxl/f3/TrVs3M3jwYDN48GDHpZazsrIcl56tWLGi6dGjh2nXrp3x9fV13OA1t0ux53Up3/yuW14udil2V5fMvViNrtYr+02E7Tf37du3rwkLCzOSTLFixcyXX36ZY1nPPfeckeS4mWxsbKwJCQkxkkytWrVyvYlwXpeGnjBhgpFkgoKCTGxsrBk8eLAZOXKkY/qtt95qJJkyZcqY7t27m+7du5syZcqYmjVrOi6J/s/Lkrta54uN159//um419N1111nunbtaqKjo3O9ifCUKVMcNwJt2LCh6dmzp4mJiTHh4eGOm0r//fffxhhjNm3aZCSZgIAA0759e9OvXz/TrVs3x/1sWrZs6XSvpbxcyqXYc7vdwPnz583tt99uJJnAwEDTo0cP0717d8dNhLt3756vG426s+3lJa/7XBlz4X5h9psDz5s3zxhz4TLowcHBRpKpXr266dOnj4mMjDTFihUz0dHRLi/1b79E+D9fQ+xyG8dJkyY5LpPeoUMHExsba0JDQ01gYKDjXleubiJsv8FvvXr1TN++fR03t/Xx8TGzZ8926m+/PHpul7C/nN/HP13OTYRbtGhh+vbta5o1a+a43HhuNxHObZyNuXDPOPvl5UNCQsztt99u+vXrZ26++WbHPenmzp3r6B8YGGi8vLxMs2bNTJ8+fUx0dLSpXbu2Y7/dsWOHo+/p06cd9/6KiIgwcXFxZvDgwea777676Lr+81LsF5PXTYSrV6/uGOfY2Fhzyy23OF47JkyYkGNZ8+bNc0wPDw93/P0rVqyYGTJkiMvf8alTpxyvlSVKlDBt27Y1ffv2NV27djU1a9Y00oWbZmfHpdhxJXDmCrhKunXrph9//FEPPvigbDabli5dqi+++EIHDx7UnXfeqY8//tjpewz9+/fXF198oRtvvFGbN2/Wl19+qcqVK2v58uW5fs69RYsWev/991W/fn199dVXeuutt/TWW285riRns9n0+eef64EHHpDNZtOiRYt05MgRvfLKK/rvf/971datIGnTpo1WrVql4OBgLVy4UNu2bdMtt9yilStXqlOnTjn6P/7441q4cKEiIiK0fv16ffbZZ/Lz89Njjz2mdevWKSgoKN81DB06VE8++aRKliypTz/9VG+99ZbjY4mS9Pnnn+uJJ55QhQoV9OWXX2rjxo2KjY3V999/rzJlylzO6jupUqWK1q9fr//85z+qWrWqli5dqi+//FKnTp3Sgw8+qC5dujj1f+CBB7RhwwYNHDhQx48f18KFC7VkyRKdOHFC/fv318KFCxUYGCjpwlUIJ0yYoA4dOmjPnj367LPP9O2336patWp6+eWXtWrVqqvyRXP79xhfeeUV1ahRQ0uWLNFXX32lOnXqaPLkyZozZ06Bu1BCdv7+/ho9erSkC9+xki5cBn39+vXq16+fMjIyNH/+fO3bt0/PPPNMrh9hc9dDDz2kuXPnKiwsTOvWrdOSJUvUpEkTff/996pZs6bLeRo1aqSUlBQNGTJEJ06c0Jw5c7R9+3Z1795d3333nfr06WNpjVfSLbfcovXr16tv3776888/NWfOHKWmpuquu+7Shg0b3Pp444033qiffvpJjz32mEqXLq1Vq1Zp3rx52r17tyIiIjRz5kzHx3Yl6bXXXlNsbKxOnTqlL7/8UosXL5aPj48SEhL0448/qlatWo6+/v7++uKLL3Trrbdq8+bNmjlzpt566y2XHw2/UuyvK48++qh8fHz02WefaePGjbrlllu0ZMkSJSQk5JinW7duWrFihTp27KgtW7boiy++UKVKlbRq1Sq1adPG5fMUL15cX375pd555x2Fh4dr27ZtmjNnjjZs2KAKFSpo3LhxV+y7aEB2NmMsvNwYABQSTz/9tMaNG6e3335bgwYN8nQ5AADgGlBw354DAAAAgEKEcAUAAAAAFiBcAQAAAIAF+M4VAAAAAFiAM1cAAAAAYAHCFQAAAABYwMfTBRREWVlZ2r9/v0qVKiWbzebpcgAAAAB4iDFGx48fV+XKlS96L0TClQv79+9XSEiIp8sAAAAAUEDs3btXVatWzbMP4cqFUqVKSbowgKVLl/ZwNQAAAAA8JT09XSEhIY6MkBfClQv2jwKWLl2acAUAAADgkr4uxAUtAAAAAMACHg9XkydPVmhoqPz9/RUeHq7k5ORc+/7888/q1auXQkNDZbPZlJSU5LLfvn37dNddd6lcuXIqXry4GjVqpA0bNlyhNQAAAAAAD4er2bNnKyEhQWPHjlVKSoqaNGmiqKgoHTx40GX/U6dOqUaNGnrhhRcUHBzsss/ff/+ttm3bqlixYvryyy+1detWTZgwQWXLlr2SqwIAAACgiLMZY4ynnjw8PFxhYWGaNGmSpAuXQA8JCdHDDz+sUaNG5TlvaGiohg8fruHDhzu1jxo1St99951Wr17tdl3p6ekKDAzUsWPH+M4VAAAAUITlJxt47MxVRkaGNm7cqMjIyP8V4+WlyMhIrV271u3lzp8/Xy1btlR0dLQqVqyoZs2aafr06XnOc/bsWaWnpzs9AAAAACA/PBauDh8+rMzMTAUFBTm1BwUFKTU11e3l/vHHH5oyZYpq1aqlJUuW6F//+peGDh2qd955J9d5EhMTFRgY6HhwjysAAAAA+eXxC1pYLSsrS82bN9fzzz+vZs2a6b777tOQIUM0derUXOcZPXq0jh075njs3bv3KlYMAAAA4FrgsXBVvnx5eXt7Ky0tzak9LS0t14tVXIpKlSqpfv36Tm316tXTnj17cp3Hz8/PcU8r7m0FAAAAwB0eC1e+vr5q0aKFli1b5mjLysrSsmXL1Lp1a7eX27ZtW23fvt2pbceOHapWrZrbywQAAACAi/Hx5JMnJCRo4MCBatmypVq1aqWkpCSdPHlScXFxkqQBAwaoSpUqSkxMlHThIhhbt251/H/fvn3avHmzSpYsqZo1a0qSHnnkEbVp00bPP/+8+vTpo+TkZE2bNk3Tpk3zzEoCAAAAKBI8eil2SZo0aZL++9//KjU1VU2bNtWrr76q8PBwSVKHDh0UGhqqmTNnSpJ27dql6tWr51hGRESEVq5c6fh54cKFGj16tH799VdVr15dCQkJGjJkyCXXxKXYAQAAAEj5ywYeD1cFEeEKAAAAgFRI7nMFAAAAANcSwhUAAAAAWIBwBQAAAAAWIFwBAAAAgAUIVwAAAABgAcIVAAAAAFiAcAUAAAAAFvDxdAGFVYsR73q6BI/Y+N8Bni4BAAAAKJA4cwUAAAAAFiBcAQAAAIAFCFcAAAAAYAHCFQAAAABYgHAFAAAAABYgXAEAAACABQhXAAAAAGAB7nOFq2bPfxp5ugSPuH7MT54uAQAAAFcBZ64AAAAAwAKEKwAAAACwAOEKAAAAACxAuAIAAAAACxCuAAAAAMAChCsAAAAAsADhCgAAAAAsQLgCAAAAAAsQrgAAAADAAoQrAAAAALAA4QoAAAAALEC4AgAAAAALEK4AAAAAwAKEKwAAAACwAOEKAAAAACxAuAIAAAAACxCuAAAAAMAChCsAAAAAsADhCgAAAAAsQLgCAAAAAAsQrgAAAADAAoQrAAAAALAA4QoAAAAALEC4AgAAAAALEK4AAAAAwAKEKwAAAACwAOEKAAAAACxAuAIAAAAACxCuAAAAAMAChCsAAAAAsECBCFeTJ09WaGio/P39FR4eruTk5Fz7/vzzz+rVq5dCQ0Nls9mUlJSU57JfeOEF2Ww2DR8+3NqiAQAAACAbj4er2bNnKyEhQWPHjlVKSoqaNGmiqKgoHTx40GX/U6dOqUaNGnrhhRcUHByc57LXr1+vN954Q40bN74SpQMAAACAg8fD1cSJEzVkyBDFxcWpfv36mjp1qgICAjRjxgyX/cPCwvTf//5XsbGx8vPzy3W5J06cUP/+/TV9+nSVLVv2SpUPAAAAAJI8HK4yMjK0ceNGRUZGOtq8vLwUGRmptWvXXtayH3roIXXu3Nlp2bk5e/as0tPTnR4AAAAAkB8eDVeHDx9WZmamgoKCnNqDgoKUmprq9nJnzZqllJQUJSYmXlL/xMREBQYGOh4hISFuPzcAAACAosnjHwu02t69ezVs2DB98MEH8vf3v6R5Ro8erWPHjjkee/fuvcJVAgAAALjW+HjyycuXLy9vb2+lpaU5taelpV30YhW52bhxow4ePKjmzZs72jIzM/XNN99o0qRJOnv2rLy9vZ3m8fPzy/P7WwAAAABwMR49c+Xr66sWLVpo2bJljrasrCwtW7ZMrVu3dmuZt9xyi3766Sdt3rzZ8WjZsqX69++vzZs35whWAAAAAGAFj565kqSEhAQNHDhQLVu2VKtWrZSUlKSTJ08qLi5OkjRgwABVqVLF8f2pjIwMbd261fH/ffv2afPmzSpZsqRq1qypUqVKqWHDhk7PUaJECZUrVy5HOwAAAABYxePhKiYmRocOHdKYMWOUmpqqpk2bavHixY6LXOzZs0deXv87wbZ//341a9bM8fNLL72kl156SREREVq5cuXVLh8AAAAAJBWAcCVJ8fHxio+Pdzntn4EpNDRUxph8LZ/QBQAAAOBKu+auFggAAAAAnkC4AgAAAAALEK4AAAAAwAKEKwAAAACwAOEKAAAAACxAuAIAAAAACxSIS7EDyF3b19p6ugSP+O7h7zxdAgAAQL5w5goAAAAALEC4AgAAAAALEK4AAAAAwAKEKwAAAACwAOEKAAAAACxAuAIAAAAACxCuAAAAAMAC3OcKwDVnVfsIT5fgERHfrPJ0CQAAFGmcuQIAAAAACxCuAAAAAMAChCsAAAAAsADhCgAAAAAsQLgCAAAAAAsQrgAAAADAAoQrAAAAALAA97kCAEiSJj26wNMleET8hDs9XQIA4BrBmSsAAAAAsADhCgAAAAAsQLgCAAAAAAsQrgAAAADAAoQrAAAAALAA4QoAAAAALEC4AgAAAAALEK4AAAAAwAKEKwAAAACwAOEKAAAAACxAuAIAAAAACxCuAAAAAMAChCsAAAAAsADhCgAAAAAsQLgCAAAAAAsQrgAAAADAAoQrAAAAALAA4QoAAAAALEC4AgAAAAALEK4AAAAAwAKEKwAAAACwAOEKAAAAACxQIMLV5MmTFRoaKn9/f4WHhys5OTnXvj///LN69eql0NBQ2Ww2JSUl5eiTmJiosLAwlSpVShUrVlT37t21ffv2K7gGAAAAAIo6j4er2bNnKyEhQWPHjlVKSoqaNGmiqKgoHTx40GX/U6dOqUaNGnrhhRcUHBzsss+qVav00EMP6fvvv9fSpUt17tw53XbbbTp58uSVXBUAAAAARZiPpwuYOHGihgwZori4OEnS1KlT9cUXX2jGjBkaNWpUjv5hYWEKCwuTJJfTJWnx4sVOP8+cOVMVK1bUxo0b1b59e4vXAAAAAAA8fOYqIyNDGzduVGRkpKPNy8tLkZGRWrt2rWXPc+zYMUnSdddd53L62bNnlZ6e7vQAAAAAgPzwaLg6fPiwMjMzFRQU5NQeFBSk1NRUS54jKytLw4cPV9u2bdWwYUOXfRITExUYGOh4hISEWPLcAAAAAIoOj3/n6kp76KGHtGXLFs2aNSvXPqNHj9axY8ccj717917FCgEAAABcCzz6navy5cvL29tbaWlpTu1paWm5XqwiP+Lj47Vw4UJ98803qlq1aq79/Pz85Ofnd9nPBwAAAKDo8uiZK19fX7Vo0ULLli1ztGVlZWnZsmVq3bq128s1xig+Pl5z587V8uXLVb16dSvKBQAAAIBcefxqgQkJCRo4cKBatmypVq1aKSkpSSdPnnRcPXDAgAGqUqWKEhMTJV24CMbWrVsd/9+3b582b96skiVLqmbNmpIufBTwww8/1Oeff65SpUo5vr8VGBio4sWLe2AtAQAAAFzrPB6uYmJidOjQIY0ZM0apqalq2rSpFi9e7LjIxZ49e+Tl9b8TbPv371ezZs0cP7/00kt66aWXFBERoZUrV0qSpkyZIknq0KGD03O9/fbbGjRo0BVdHwBA0fHcXb09XYJHPPH+HE+XAAAFksfDlXThu1Hx8fEup9kDk11oaKiMMXku72LTAQAAAMBq1/zVAgEAAADgaiBcAQAAAIAFCFcAAAAAYAHCFQAAAABYgHAFAAAAABYgXAEAAACABQhXAAAAAGABwhUAAAAAWIBwBQAAAAAWIFwBAAAAgAUIVwAAAABgAcIVAAAAAFiAcAUAAAAAFvDxdAEAAKBo2fbcck+X4BH1nrjZ0yUAuMI4cwUAAAAAFiBcAQAAAIAFCFcAAAAAYAHCFQAAAABYgHAFAAAAABYgXAEAAACABQhXAAAAAGABwhUAAAAAWIBwBQAAAAAWIFwBAAAAgAUIVwAAAABgAcIVAAAAAFiAcAUAAAAAFiBcAQAAAIAFCFcAAAAAYAHCFQAAAABYgHAFAAAAABYgXAEAAACABQhXAAAAAGABwhUAAAAAWIBwBQAAAAAWIFwBAAAAgAUIVwAAAABgAcIVAAAAAFiAcAUAAAAAFiBcAQAAAIAFCFcAAAAAYAHCFQAAAABYgHAFAAAAABYgXAEAAACABQhXAAAAAGABwhUAAAAAWKBAhKvJkycrNDRU/v7+Cg8PV3Jycq59f/75Z/Xq1UuhoaGy2WxKSkq67GUCAAAAwOXyeLiaPXu2EhISNHbsWKWkpKhJkyaKiorSwYMHXfY/deqUatSooRdeeEHBwcGWLBMAAAAALpfHw9XEiRM1ZMgQxcXFqX79+po6daoCAgI0Y8YMl/3DwsL03//+V7GxsfLz87NkmQAAAABwuTwarjIyMrRx40ZFRkY62ry8vBQZGam1a9detWWePXtW6enpTg8AAAAAyA+PhqvDhw8rMzNTQUFBTu1BQUFKTU29astMTExUYGCg4xESEuLWcwMAAAAoujz+scCCYPTo0Tp27JjjsXfvXk+XBAAAAKCQ8fHkk5cvX17e3t5KS0tzak9LS8v1YhVXYpl+fn65fn8LAAAAAC6FR89c+fr6qkWLFlq2bJmjLSsrS8uWLVPr1q0LzDIBAAAA4GI8euZKkhISEjRw4EC1bNlSrVq1UlJSkk6ePKm4uDhJ0oABA1SlShUlJiZKunDBiq1btzr+v2/fPm3evFklS5ZUzZo1L2mZAAAAAGA1j4ermJgYHTp0SGPGjFFqaqqaNm2qxYsXOy5IsWfPHnl5/e8E2/79+9WsWTPHzy+99JJeeuklRUREaOXKlZe0TAAAAACwmsfDlSTFx8crPj7e5TR7YLILDQ2VMeaylgkAAAAAVuNqgQAAAABgAcIVAAAAAFiAcAUAAAAAFiBcAQAAAIAFCFcAAAAAYAHCFQAAAABYgHAFAAAAABYgXAEAAACABQhXAAAAAGABwhUAAAAAWMDtcPXee++pbdu2qly5snbv3i1JSkpK0ueff25ZcQAAAABQWLgVrqZMmaKEhATdcccdOnr0qDIzMyVJZcqUUVJSkpX1AQAAAECh4Fa4eu211zR9+nQ98cQT8vb2drS3bNlSP/30k2XFAQAAAEBh4Va42rlzp5o1a5aj3c/PTydPnrzsogAAAACgsHErXFWvXl2bN2/O0b548WLVq1fvcmsCAAAAgELHx52ZEhIS9NBDD+nMmTMyxig5OVkfffSREhMT9eabb1pdIwAAAAAUeG6Fq3vvvVfFixfXk08+qVOnTqlfv36qXLmyXnnlFcXGxlpdIwAAAAAUeG6FK0nq37+/+vfvr1OnTunEiROqWLGilXUBAAAAQKHiVrjauXOnzp8/r1q1aikgIEABAQGSpF9//VXFihVTaGiolTUCAAAAQIHn1gUtBg0apDVr1uRoX7dunQYNGnS5NQEAAABAoeNWuNq0aZPatm2bo/3GG290eRVBAAAAALjWuRWubDabjh8/nqP92LFjyszMvOyiAAAAAKCwcStctW/fXomJiU5BKjMzU4mJibrpppssKw4AAAAACgu3Lmgxfvx4tW/fXnXq1FG7du0kSatXr1Z6erqWL19uaYEAAAAAUBi4deaqfv36+vHHH9WnTx8dPHhQx48f14ABA/TLL7+oYcOGVtcIAAAAAAWe2/e5qly5sp5//nkrawEAAACAQsvtcHX06FElJyfr4MGDysrKcpo2YMCAyy4MAAAAAAoTt8LVggUL1L9/f504cUKlS5eWzWZzTLPZbIQrAAAAAEWOW9+5evTRR3XPPffoxIkTOnr0qP7++2/H48iRI1bXCAAAAAAFnlvhat++fRo6dKgCAgKsrgcAAAAACiW3wlVUVJQ2bNhgdS0AAAAAUGi59Z2rzp07a8SIEdq6dasaNWqkYsWKOU3v2rWrJcUBAAAAQGHhVrgaMmSIJOk///lPjmk2m02ZmZmXVxUAAAAAFDJuhat/XnodAAAAAIo6t75zBQAAAABw5vZNhE+ePKlVq1Zpz549ysjIcJo2dOjQyy4MAAAAAAoTt8LVpk2bdMcdd+jUqVM6efKkrrvuOh0+fFgBAQGqWLEi4QoAAABAkePWxwIfeeQR3Xnnnfr7779VvHhxff/999q9e7datGihl156yeoaAQAAAKDAcytcbd68WY8++qi8vLzk7e2ts2fPKiQkRC+++KIef/xxq2sEAAAAgALPrXBVrFgxeXldmLVixYras2ePJCkwMFB79+61rjoAAAAAKCTc+s5Vs2bNtH79etWqVUsREREaM2aMDh8+rPfee08NGza0ukYAAAAAKPDcOnP1/PPPq1KlSpKk5557TmXLltW//vUvHTp0SG+88YalBQIAAABAYeDWmauWLVs6/l+xYkUtXrzYsoIAAAAAoDBy68zVzTffrKNHj+ZoT09P180333y5NQEAAABAoeNWuFq5cmWOGwdL0pkzZ7R69erLLgoAAAAACpt8fSzwxx9/dPx/69atSk1NdfycmZmpxYsXq0qVKtZVBwAAAACFRL7OXDVt2lTNmjWTzWbTzTffrKZNmzoeLVq00LPPPqsxY8bku4jJkycrNDRU/v7+Cg8PV3Jycp79P/nkE9WtW1f+/v5q1KiRFi1a5DT9xIkTio+PV9WqVVW8eHHVr19fU6dOzXddAAAAAHCp8nXmaufOnTLGqEaNGkpOTlaFChUc03x9fVWxYkV5e3vnq4DZs2crISFBU6dOVXh4uJKSkhQVFaXt27erYsWKOfqvWbNGffv2VWJiorp06aIPP/xQ3bt3V0pKiuMy8AkJCVq+fLnef/99hYaG6quvvtKDDz6oypUrq2vXrvmqDwAAAAAuRb7OXFWrVk1VqlTRwIEDVa5cOVWrVs3xqFSpUr6DlSRNnDhRQ4YMUVxcnOMMU0BAgGbMmOGy/yuvvKJOnTppxIgRqlevnp555hk1b95ckyZNcvRZs2aNBg4cqA4dOig0NFT33XefmjRpctEzYgAAAADgrnxf0KJYsWKaO3euJU+ekZGhjRs3KjIy8n8FeXkpMjJSa9eudTnP2rVrnfpLUlRUlFP/Nm3aaP78+dq3b5+MMVqxYoV27Nih2267zeUyz549q/T0dKcHAAAAAOSHW1cL7Natm+bNm3fZT3748GFlZmYqKCjIqT0oKMjpYhnZpaamXrT/a6+9pvr166tq1ary9fVVp06dNHnyZLVv397lMhMTExUYGOh4hISEXOaaAQAAAChq3LqJcK1atfSf//xH3333nVq0aKESJUo4TR86dKglxbnrtdde0/fff6/58+erWrVq+uabb/TQQw+pcuXKOc56SdLo0aOVkJDg+Dk9PZ2ABQAAACBf3ApXb731lsqUKaONGzdq48aNTtNsNtslh6vy5cvL29tbaWlpTu1paWkKDg52OU9wcHCe/U+fPq3HH39cc+fOVefOnSVJjRs31ubNm/XSSy+5DFd+fn7y8/O7pJoBAAAAwBW3Pha4c+fOXB9//PHHJS/H19dXLVq00LJlyxxtWVlZWrZsmVq3bu1yntatWzv1l6SlS5c6+p87d07nzp2Tl5fzqnl7eysrK+uSawMAAACA/HDrzFV2xhhJF85YuSMhIUEDBw5Uy5Yt1apVKyUlJenkyZOKi4uTJA0YMEBVqlRRYmKiJGnYsGGKiIjQhAkT1LlzZ82aNUsbNmzQtGnTJEmlS5dWRESERowYoeLFi6tatWpatWqV3n33XU2cOPFyVxcAAAAAXHLrzJUkvfvuu2rUqJGKFy+u4sWLq3HjxnrvvffyvZyYmBi99NJLGjNmjJo2barNmzdr8eLFjotW7NmzRwcOHHD0b9OmjT788ENNmzZNTZo00Zw5czRv3jzHPa4kadasWQoLC1P//v1Vv359vfDCC3ruuef0wAMPuLu6AAAAAJAnt85cTZw4UU899ZTi4+PVtm1bSdK3336rBx54QIcPH9YjjzySr+XFx8crPj7e5bSVK1fmaIuOjlZ0dHSuywsODtbbb7+drxoAAAAA4HK4Fa5ee+01TZkyRQMGDHC0de3aVQ0aNNDTTz+d73AFAAAAAIWdWx8LPHDggNq0aZOjvU2bNk4f4QMAAACAosKtcFWzZk19/PHHOdpnz56tWrVqXXZRAAAAAFDYuPWxwHHjxikmJkbffPON4ztX3333nZYtW+YydAEAAADAtc6tM1e9evXSunXrVL58ec2bN0/z5s1T+fLllZycrB49elhdIwAAAAAUeG7f56pFixZ6//33rawFAAAAAAott8NVZmam5s6dq23btkmS6tevr27dusnH57LvSwwAAAAAhY5bSejnn39W165dlZqaqjp16kiSxo8frwoVKmjBggVON/QFAAAAgKLAre9c3XvvvWrQoIH+/PNPpaSkKCUlRXv37lXjxo113333WV0jAAAAABR4bp252rx5szZs2KCyZcs62sqWLavnnntOYWFhlhUHAAAAAIWFW2euateurbS0tBztBw8eVM2aNS+7KAAAAAAobNwKV4mJiRo6dKjmzJmjP//8U3/++afmzJmj4cOHa/z48UpPT3c8AAAAAKAocOtjgV26dJEk9enTRzabTZJkjJEk3XnnnY6fbTabMjMzragTAAAAAAo0t8LVihUrrK4DAAAAAAo1t8JVRESE1XUAAAAAQKHm9h1/z5w5ox9//FEHDx5UVlaW07SuXbtedmEAAAAAUJi4Fa4WL16sAQMG6PDhwzmm8T0rAAAAAEWRW1cLfPjhhxUdHa0DBw4oKyvL6UGwAgAAAFAUuRWu0tLSlJCQoKCgIKvrAQAAAIBCya1w1bt3b61cudLiUgAAAACg8HLrO1eTJk1SdHS0Vq9erUaNGqlYsWJO04cOHWpJcQAAAABQWLgVrj766CN99dVX8vf318qVKx03EpYuXNCCcAUAAACgqHErXD3xxBMaN26cRo0aJS8vtz5ZCAAAAADXFLeSUUZGhmJiYghWAAAAAPD/3EpHAwcO1OzZs62uBQAAAAAKLbc+FpiZmakXX3xRS5YsUePGjXNc0GLixImWFAcAAAAAhYVb4eqnn35Ss2bNJElbtmyxtCAAAAAAKIzcClcrVqywug4AAAAAKNTyFa569ux50T42m02ffvqp2wUBAAAAQGGUr3AVGBh4peoAAAAAgEItX+Hq7bffvlJ1AAAAAEChxo2qAAAAAMAChCsAAAAAsADhCgAAAAAsQLgCAAAAAAsQrgAAAADAAoQrAAAAALAA4QoAAAAALEC4AgAAAAALEK4AAAAAwAKEKwAAAACwAOEKAAAAACxAuAIAAAAACxCuAAAAAMAChCsAAAAAsADhCgAAAAAsUCDC1eTJkxUaGip/f3+Fh4crOTk5z/6ffPKJ6tatK39/fzVq1EiLFi3K0Wfbtm3q2rWrAgMDVaJECYWFhWnPnj1XahUAAAAAFHEeD1ezZ89WQkKCxo4dq5SUFDVp0kRRUVE6ePCgy/5r1qxR3759NXjwYG3atEndu3dX9+7dtWXLFkef33//XTfddJPq1q2rlStX6scff9RTTz0lf3//q7VaAAAAAIoYj4eriRMnasiQIYqLi1P9+vU1depUBQQEaMaMGS77v/LKK+rUqZNGjBihevXq6ZlnnlHz5s01adIkR58nnnhCd9xxh1588UU1a9ZMN9xwg7p27aqKFSterdUCAAAAUMR4NFxlZGRo48aNioyMdLR5eXkpMjJSa9eudTnP2rVrnfpLUlRUlKN/VlaWvvjiC9WuXVtRUVGqWLGiwsPDNW/evFzrOHv2rNLT050eAAAAAJAfHg1Xhw8fVmZmpoKCgpzag4KClJqa6nKe1NTUPPsfPHhQJ06c0AsvvKBOnTrpq6++Uo8ePdSzZ0+tWrXK5TITExMVGBjoeISEhFiwdgAAAACKEo9/LNBqWVlZkqRu3brpkUceUdOmTTVq1Ch16dJFU6dOdTnP6NGjdezYMcdj7969V7NkAAAAANcAH08+efny5eXt7a20tDSn9rS0NAUHB7ucJzg4OM/+5cuXl4+Pj+rXr+/Up169evr2229dLtPPz09+fn7urgYAAAAAePbMla+vr1q0aKFly5Y52rKysrRs2TK1bt3a5TytW7d26i9JS5cudfT39fVVWFiYtm/f7tRnx44dqlatmsVrAAAAAAAXePTMlSQlJCRo4MCBatmypVq1aqWkpCSdPHlScXFxkqQBAwaoSpUqSkxMlCQNGzZMERERmjBhgjp37qxZs2Zpw4YNmjZtmmOZI0aMUExMjNq3b6+OHTtq8eLFWrBggVauXOmJVQQAAABQBHg8XMXExOjQoUMaM2aMUlNT1bRpUy1evNhx0Yo9e/bIy+t/J9jatGmjDz/8UE8++aQef/xx1apVS/PmzVPDhg0dfXr06KGpU6cqMTFRQ4cOVZ06dfTpp5/qpptuuurrBwAAAKBo8Hi4kqT4+HjFx8e7nObqbFN0dLSio6PzXOY999yje+65x4ryAAAAAOCirrmrBQIAAACAJxCuAAAAAMAChCsAAAAAsADhCgAAAAAsQLgCAAAAAAsQrgAAAADAAoQrAAAAALAA4QoAAAAALEC4AgAAAAALEK4AAAAAwAKEKwAAAACwAOEKAAAAACxAuAIAAAAACxCuAAAAAMAChCsAAAAAsADhCgAAAAAsQLgCAAAAAAsQrgAAAADAAoQrAAAAALAA4QoAAAAALEC4AgAAAAALEK4AAAAAwAKEKwAAAACwAOEKAAAAACxAuAIAAAAACxCuAAAAAMAChCsAAAAAsADhCgAAAAAsQLgCAAAAAAsQrgAAAADAAoQrAAAAALAA4QoAAAAALEC4AgAAAAALEK4AAAAAwAKEKwAAAACwAOEKAAAAACxAuAIAAAAACxCuAAAAAMAChCsAAAAAsADhCgAAAAAsQLgCAAAAAAsQrgAAAADAAoQrAAAAALAA4QoAAAAALEC4AgAAAAALFIhwNXnyZIWGhsrf31/h4eFKTk7Os/8nn3yiunXryt/fX40aNdKiRYty7fvAAw/IZrMpKSnJ4qoBAAAA4H88Hq5mz56thIQEjR07VikpKWrSpImioqJ08OBBl/3XrFmjvn37avDgwdq0aZO6d++u7t27a8uWLTn6zp07V99//70qV658pVcDAAAAQBHn8XA1ceJEDRkyRHFxcapfv76mTp2qgIAAzZgxw2X/V155RZ06ddKIESNUr149PfPMM2revLkmTZrk1G/fvn16+OGH9cEHH6hYsWJXY1UAAAAAFGEeDVcZGRnauHGjIiMjHW1eXl6KjIzU2rVrXc6zdu1ap/6SFBUV5dQ/KytLd999t0aMGKEGDRpctI6zZ88qPT3d6QEAAAAA+eHRcHX48GFlZmYqKCjIqT0oKEipqaku50lNTb1o//Hjx8vHx0dDhw69pDoSExMVGBjoeISEhORzTQAAAAAUdR7/WKDVNm7cqFdeeUUzZ86UzWa7pHlGjx6tY8eOOR579+69wlUCAAAAuNZ4NFyVL19e3t7eSktLc2pPS0tTcHCwy3mCg4Pz7L969WodPHhQ119/vXx8fOTj46Pdu3fr0UcfVWhoqMtl+vn5qXTp0k4PAAAAAMgPj4YrX19ftWjRQsuWLXO0ZWVladmyZWrdurXLeVq3bu3UX5KWLl3q6H/33Xfrxx9/1ObNmx2PypUra8SIEVqyZMmVWxkAAAAARZqPpwtISEjQwIED1bJlS7Vq1UpJSUk6efKk4uLiJEkDBgxQlSpVlJiYKEkaNmyYIiIiNGHCBHXu3FmzZs3Shg0bNG3aNElSuXLlVK5cOafnKFasmIKDg1WnTp2ru3IAAAAAigyPh6uYmBgdOnRIY8aMUWpqqpo2barFixc7LlqxZ88eeXn97wRbmzZt9OGHH+rJJ5/U448/rlq1amnevHlq2LChp1YBAAAAADwfriQpPj5e8fHxLqetXLkyR1t0dLSio6Mvefm7du1yszIAAAAAuDTX3NUCAQAAAMATCFcAAAAAYAHCFQAAAABYgHAFAAAAABYgXAEAAACABQhXAAAAAGABwhUAAAAAWIBwBQAAAAAWIFwBAAAAgAUIVwAAAABgAcIVAAAAAFiAcAUAAAAAFiBcAQAAAIAFCFcAAAAAYAHCFQAAAABYgHAFAAAAABYgXAEAAACABQhXAAAAAGABwhUAAAAAWIBwBQAAAAAWIFwBAAAAgAUIVwAAAABgAcIVAAAAAFiAcAUAAAAAFiBcAQAAAIAFCFcAAAAAYAHCFQAAAABYgHAFAAAAABYgXAEAAACABQhXAAAAAGABwhUAAAAAWIBwBQAAAAAWIFwBAAAAgAUIVwAAAABgAcIVAAAAAFiAcAUAAAAAFiBcAQAAAIAFfDxdAAAAAPL29NNPe7oEjyiq643CizNXAAAAAGABwhUAAAAAWICPBQIAAOCa9PEnrTxdgkf0iU72dAlFFmeuAAAAAMAChCsAAAAAsADhCgAAAAAsQLgCAAAAAAsQrgAAAADAAgUiXE2ePFmhoaHy9/dXeHi4kpPzvsLJJ598orp168rf31+NGjXSokWLHNPOnTunkSNHqlGjRipRooQqV66sAQMGaP/+/Vd6NQAAAAAUYR4PV7Nnz1ZCQoLGjh2rlJQUNWnSRFFRUTp48KDL/mvWrFHfvn01ePBgbdq0Sd27d1f37t21ZcsWSdKpU6eUkpKip556SikpKfrss8+0fft2de3a9WquFgAAAIAixuPhauLEiRoyZIji4uJUv359TZ06VQEBAZoxY4bL/q+88oo6deqkESNGqF69enrmmWfUvHlzTZo0SZIUGBiopUuXqk+fPqpTp45uvPFGTZo0SRs3btSePXuu5qoBAAAAKEI8Gq4yMjK0ceNGRUZGOtq8vLwUGRmptWvXupxn7dq1Tv0lKSoqKtf+knTs2DHZbDaVKVPG5fSzZ88qPT3d6QEAAAAA+eHRcHX48GFlZmYqKCjIqT0oKEipqaku50lNTc1X/zNnzmjkyJHq27evSpcu7bJPYmKiAgMDHY+QkBA31gYAAABAUebxjwVeSefOnVOfPn1kjNGUKVNy7Td69GgdO3bM8di7d+9VrBIAAADAtcDHk09evnx5eXt7Ky0tzak9LS1NwcHBLucJDg6+pP72YLV7924tX74817NWkuTn5yc/Pz831wIAAAAAPHzmytfXVy1atNCyZcscbVlZWVq2bJlat27tcp7WrVs79ZekpUuXOvW3B6tff/1VX3/9tcqVK3dlVgAAAAAA/p9Hz1xJUkJCggYOHKiWLVuqVatWSkpK0smTJxUXFydJGjBggKpUqaLExERJ0rBhwxQREaEJEyaoc+fOmjVrljZs2KBp06ZJuhCsevfurZSUFC1cuFCZmZmO72Ndd9118vX19cyKAgAAALimeTxcxcTE6NChQxozZoxSU1PVtGlTLV682HHRij179sjL638n2Nq0aaMPP/xQTz75pB5//HHVqlVL8+bNU8OGDSVJ+/bt0/z58yVJTZs2dXquFStWqEOHDldlvQAAAAAULR4PV5IUHx+v+Ph4l9NWrlyZoy06OlrR0dEu+4eGhsoYY2V5AAAAAHBR1/TVAgEAAADgaiFcAQAAAIAFCFcAAAAAYAHCFQAAAABYgHAFAAAAABYgXAEAAACABQrEpdgBAAAAeF6TOUs8XYJH/NA7ypLlcOYKAAAAACxAuAIAAAAACxCuAAAAAMAChCsAAAAAsADhCgAAAAAsQLgCAAAAAAsQrgAAAADAAoQrAAAAALAA4QoAAAAALEC4AgAAAAALEK4AAAAAwAKEKwAAAACwAOEKAAAAACxAuAIAAAAACxCuAAAAAMAChCsAAAAAsADhCgAAAAAsQLgCAAAAAAsQrgAAAADAAoQrAAAAALAA4QoAAAAALEC4AgAAAAALEK4AAAAAwAKEKwAAAACwAOEKAAAAACxAuAIAAAAACxCuAAAAAMAChCsAAAAAsADhCgAAAAAsQLgCAAAAAAsQrgAAAADAAoQrAAAAALAA4QoAAAAALEC4AgAAAAALEK4AAAAAwAKEKwAAAACwAOEKAAAAACxAuAIAAAAACxCuAAAAAMACBSJcTZ48WaGhofL391d4eLiSk5Pz7P/JJ5+obt268vf3V6NGjbRo0SKn6cYYjRkzRpUqVVLx4sUVGRmpX3/99UquAgAAAIAizuPhavbs2UpISNDYsWOVkpKiJk2aKCoqSgcPHnTZf82aNerbt68GDx6sTZs2qXv37urevbu2bNni6PPiiy/q1Vdf1dSpU7Vu3TqVKFFCUVFROnPmzNVaLQAAAABFjMfD1cSJEzVkyBDFxcWpfv36mjp1qgICAjRjxgyX/V955RV16tRJI0aMUL169fTMM8+oefPmmjRpkqQLZ62SkpL05JNPqlu3bmrcuLHeffdd7d+/X/PmzbuKawYAAACgKPHx5JNnZGRo48aNGj16tKPNy8tLkZGRWrt2rct51q5dq4SEBKe2qKgoR3DauXOnUlNTFRkZ6ZgeGBio8PBwrV27VrGxsTmWefbsWZ09e9bx87FjxyRJ6enpudaeefb0xVfwGpTXmFzM8TOZFlZSeFzOmEnS+dPnLaqkcLmccTt5njFzx+mzpyyqpHC5nHE7c+6chZUUHpe7rZ04c9KiSgqXyxm37McpRcnlbmunTnHskV+Zp9g/c5tmjLnocjwarg4fPqzMzEwFBQU5tQcFBemXX35xOU9qaqrL/qmpqY7p9rbc+vxTYmKixo0bl6M9JCTk0lakCAl87QFPl1D4JAZ6uoJCKXAk45ZvgYyZOx6b7OkKCp9nP2Zbc8uzni6g8HnhhRc8XUKhFDeIfTS/LmXEjh8/rsCL/K31aLgqKEaPHu10NiwrK0tHjhxRuXLlZLPZPFhZTunp6QoJCdHevXtVunRpT5dTKDBm7mHc8o8xcw/jln+MmXsYt/xjzNzDuOVfQR4zY4yOHz+uypUrX7SvR8NV+fLl5e3trbS0NKf2tLQ0BQcHu5wnODg4z/72f9PS0lSpUiWnPk2bNnW5TD8/P/n5+Tm1lSlTJj+rctWVLl26wG14BR1j5h7GLf8YM/cwbvnHmLmHccs/xsw9jFv+FdQxu9gZKzuPXtDC19dXLVq00LJlyxxtWVlZWrZsmVq3bu1yntatWzv1l6SlS5c6+levXl3BwcFOfdLT07Vu3bpclwkAAAAAl8vjHwtMSEjQwIED1bJlS7Vq1UpJSUk6efKk4uLiJEkDBgxQlSpVlJiYKEkaNmyYIiIiNGHCBHXu3FmzZs3Shg0bNG3aNEmSzWbT8OHD9eyzz6pWrVqqXr26nnrqKVWuXFndu3f31GoCAAAAuMZ5PFzFxMTo0KFDGjNmjFJTU9W0aVMtXrzYcUGKPXv2yMvrfyfY2rRpow8//FBPPvmkHn/8cdWqVUvz5s1Tw4YNHX0ee+wxnTx5Uvfdd5+OHj2qm266SYsXL5a/v/9VXz+r+fn5aezYsTk+xojcMWbuYdzyjzFzD+OWf4yZexi3/GPM3MO45d+1MmY2cynXFAQAAAAA5MnjNxEGAAAAgGsB4QoAAAAALEC4AgAAAAALEK4AAAAAwAKEKwAAAACwAOEKAAAAACxAuCogsrKyPF1CoZP9LgLcUSBvjI977Psl+2f+ZWZmeroEXON4XXMf+6d7+FvgnqI2boSrAsJ+o+S0tDRJRW9DzC9jjGw2m6QLfyTs/0dOWVlZstlsOn78uI4dO6b9+/d7uqRCwRgjLy8v/fbbb5o2bZoOHjzo6ZIKjaysLHl7e0uSFi1apH379nm4ooLN/npPWLh09te1w4cPa9u2bUpJSfF0SYVG9v3z3Xff1ezZsz1cUeFhP1Y7evSoZwspZOzjtmbNGp04ceKaf60jXBUgs2bNUoMGDXTmzBnHhgjX7GEqKSlJTz75pCQOTFzJysqSl5eXtm7dqp49e6p9+/Zq0qSJXn/9dUmMWV5sNpuOHTumO+64Q08//bRmzZqlw4cPe7qsAs++zUnS448/ri5dumjSpEkerqrgso/Xtm3b9J///EfHjx/3dEkFnn3MtmzZosjISPXt21ctW7bUv//9b0+XVuBl3z9HjBihQYMG6Z133vFwVYXLG2+8oZ49e/ImeD4YY/TNN9/o5ptv1rFjx2Sz2a7p4w+O4AuQsLAwXX/99Zo7d64kDnwvxeLFix3vIHH2ypn9zMu2bdvUrl07NW3aVCNHjtS///1vxcfHa8mSJYzZRfj6+qpy5cqqXLmyXn75Zb333nuOs8v2/ZP99H+yH7gNGzZMb7zxhnr16qX09HQPV1ZweXl56ffff1dERITGjRunxx57TKdPn/Z0WQWW/XVt69atioiI0O23364PPvhAH3/8sSZOnKhffvnF0yUWWNn3z0ceeUQzZ87U888/rzNnzujs2bOEhUv0119/ObZDXBqbzab27durWbNmGjdunOPM87WKLcNDXB2QVapUSZUqVdJnn30mibDwT65e+G+++Wbt2bNHmZmZ/GH4B5vNpiNHjmjYsGEaMGCA/vvf/6pfv34aOXKkunTposWLF0viI6i5yczMVPHixdWwYUO9+eabuueee5SUlKRPPvlEkhxvgrCf/o/9YOPhhx/WO++8o+TkZEVGRmrNmjU6f/68h6srmE6cOKGXXnpJN998sz744AO99957evjhhwlYubDZbDp06JAeeughDRgwQImJiWrQoIG6du2qqKgo/fXXX1q1apUOHDjg6VILHPv++a9//UszZ87UihUr1KFDB/3www98YiYXrv4+tmnTRrt379aff/7JR3pz8c9xy8jIkCT16dNH27dvd3wK5FodN/YkD7EfkKWmpjraAgIC9Oyzz2rp0qWaN2+ehyoruOwv/PPmzdOOHTt08OBBBQQEaMeOHTpx4kSOPwzX6k57KRYtWqSffvpJWVlZOnr0qG655Ran6dWrV9dvv/3moeoKrkWLFunHH3+UJMd3Evz9/bVw4UI99dRTio6O1ssvv6xWrVrpgQce0JEjRzxZboE0f/58ffXVV1qxYoVuuOEG+fv76/z58/Lx8fF0aQWKfR8tUaKE6tevr549e6pv376aP3++Zs2aRcByYdGiRdqyZYt8fX3VoUMHDRkyxDFt/Pjx+uqrr/TII4+oR48eGjRokFauXOm5YguQ7H8Lt27dql9++UXLly9Xw4YNVaJECQUEBDgOfuHMflwxZcoUzZo1S99++612794tb29vZWRkOKbzJpsz+7gkJydLuvApEEnq27evfv75Z73xxhuSruFxM/CYyZMnm+bNm5v777/fpKWlmZMnTxpjjOndu7cZMWKEMcaYzMxMT5ZY4HzzzTemSpUqpnLlyqZEiRLmlltuMX5+fub+++83CxcuNN99953JzMw0J06c8HSpHpOammqqV69uBg0aZHbv3m22bt3qmJaRkWGMMWbs2LGmR48eTvOdPn36qtZZ0NjHLS4uzvz888+O9qSkJNOzZ0/Hzw0aNDDFihUzjz76qDl27JgnSi3Q/vrrL7N//37Hz+vXrzc1a9Y0f/31l6MtKSnJ7Nu3zxPlFQjZt7WdO3ea8+fPO01funSpKVGihBk8eLA5deqUMcaY8+fPmx07dnii3AIh++vanj17zLlz5xzTPv/8c+Pr62vmzJlj/v77b/Prr7+a2rVrO/6O4oIjR44YY4zT38cjR46YChUqmOTkZEdbQkKC+eKLL656fQVJVlaW4//79+83YWFhJjw83JQsWdK0bdvW2Gw206xZM5OYmGjeeOMNs3HjxiK9f9plH7cvvvjChIaGmubNm5tFixaZ33//3Rhz4fW/ffv2jp+vRYQrD/rtt99MYmKiufHGG01oaKi5//77zQ8//GBmzpxpAgMDza5duzxdYoFz8uRJk5WVZXbv3m1Wrlxppk+fbnx8fExYWJgJCQkxJUqUMFWqVDH333+/p0v1qI0bN5qWLVuae++912zZssUYc+HgzB7Wn3vuOXPrrbc6+j/11FPmhRdeKPJhfuPGjSYsLMxp3FJSUsydd95pjDHm7rvvNlWqVDF33323qVWrlnnuueecQkNR88/txR4Ssv+B3bRpkylZsqQ5cOCAMcaY2267zVSuXDlHoChqXO2jWVlZjrHLHrD+/vtv89BDD5k+ffqY48ePe7Jsj3I1ZpmZmebnn382P/30kzHmf9veXXfdZaKiopy2xaIm+/45efJk069fP8e4GXNhfz18+LApV66cWbt2rTHGmKioKFOtWjWn8FqUHT582OnnXbt2mW3btpnatWubOnXqmC5duphKlSqZSpUqmdtvv91DVRY8x48fNydPnjQpKSkmNjbWtGjRwtSrV8+8+eabZsaMGaZu3bpm+fLlxphr8yQC4eoq+efGYz+DYH/hf/31182AAQOMn5+fue+++4zNZjOPPfZYkX6By+3ALfv///rrL1O7dm0zf/58c+rUKfPrr7+aBQsWFPkDN2MuhILmzZube++91+lMjDEXwlVERIQxxpgnn3zS2Gw2s2HDBg9UWfBkH7dffvnF7Nu3z9SpU8dERESY4OBgx8HJAw88YBo3blxkw1X2/XPGjBlm+PDhZvDgwWbNmjWO9vPnz5tff/3VVK1a1fz222+me/fupl69eo7Xv2vxj2p+5LaP2sfl66+/NmXKlDHVqlUz3t7eZuPGjZ4qtcDIPmbZg0J2GRkZJjo62owZM+YqV1dwZN+3UlJSzD333GNKlixp7rvvPvPLL78YYy4cf5w+fdq0aNHCLFiwwHTr1s3UrVvXsX8Wxb+j/wyk/fv3N5s3b3a0ZWVlmczMTNOzZ0/z/PPPG2MuvOm7b9++Ijledv8ct759+zp9amb9+vVm4sSJJjQ01MTGxhqbzWbatm17zX76g3B1FeR2EPLdd9/l6LtixQrzwAMPmOrVq5vatWs7wlVRe/ftUg7c7GMSHR1tnnjiiRzLKMovdHa5HYiMGzfO9OvXz0yYMMH4+flx0PYPKSkpplmzZmbw4MHmxx9/NPfee69p1KiRWb9+vVO/tLQ0D1VYcIwcOdJUrVrVxMbGmrvvvtt4eXmZjz76yDH9zJkzpkaNGqZ06dKmZs2ajgO3ovzGUXa57aP217fOnTubcuXKOc7M4OIB68knnzQhISF8TMtc+IjfDTfcYIYNG2Z69eplvLy8TFxcnNm2bZujz4033mhsNpupU6dOkd4/LyWQ2j333HOmffv25ty5c07zFcXjjrzG7Z/757Zt28ycOXPM7bffboKCgsyqVatyLONaQLi6inI7CPnnR2kyMjLM/v37TUhIiHnuuec8WbLHXezAzRhj4uPjHWdhkFP2AxH7Adr48eONzWYzZcuWzREYcIF93O6//37zzjvvOL6vYEzR/APqyowZM0xISIhjG/ryyy+NzWYzvr6+5o033jCZmZkmPT3dNG7c2ISFhTkO2IrigVteXIWFzMxM88QTTxibzeb0zjkucDVms2fPNgMHDjQVK1Y0KSkpHq7Q81auXGnKlStnvv/+e0fbJ598Yq677jozcOBAx9nSxx9/3Nx5553sn/8vt0Ca/UzMjBkzTNWqVT1YZcFzKeOW3U033WSio6OvcpVXB+HqKrnYQUh29gT/0EMPFenvDl3KgZsxxrzwwgumV69eRe7sXn5kPxDZuXOn+eqrr0yJEiV4N/wi7N/BGjJkSK5/IIqqkydPmvHjx5vp06cbY4yZP3++KVWqlJk2bZp54oknjJ+fn3nnnXeMMRcuRmAPpEX9wC03rj4i+Oabb7KP5iH7mO3YscPx/Y5/fgy6qFq5cqUJCQkxv/zyi9P3+T766CNjs9nMAw88YPbs2WNOnTrlmFbU98+LBVL734GVK1eaLl26XHNnXNx1sXHLfubvzJkzxpgL22Hr1q3N0aNHr3q9Vxrh6iq4lIOQ999/P0c4iI6ONrfccovJyMgocsHhUsfMmAsfzXL1RXo4S0lJMWFhYSYmJsZs3779mnxBuxJSUlJMq1atTGxsrNNHaWDML7/8Ynbu3Gn++OMPU69ePZOUlGSMMWbNmjXGZrMZm81m5syZ4+jPGb+82be1mJgY88cff3i6nEIh+5jt2bPH8bE2XNgPAwICHBcOsB/UHjt2zFx//fUmKCjIDB061HH1QILCxQPpgw8+aHbs2OF0XMa4Xdq4/fNNj7i4OFO/fv1r8iI9hKur5FIOQubOnevov337dtO2bdsifZGBSxmzTz/91NGfF7iLS05ONhEREU6XysbFMW55W758uWnWrJkjEGzatMkMHz7cvPfee0X+nfD8YlvLv+TkZNO+fXvGzIV77rnHlC1b1umNoUOHDpl//etfZtKkScbLy8t8+eWXHqywYCGQuudSxm348OHm+PHjjgup9O3b1+lM17WEcHWVXepByNmzZzmz8P84cLNWUb+flbsYt9x98cUXxmazmcWLF5tff/3VdOnSxfTp08cxnf00f9jW8o8xc+2PP/4w3bp1MyVKlDCvvPKKmT59urn11ltNu3btjDHGNGnSxCQkJHi4yoKFQOqe/I7btRxMfTx9E+Oi5vTp09q8ebN27NihzMxMPfXUUwoICNDLL78sSTp//rx8fHzk6+vruKN1UXepY4ZL4+/v7+kSCqWiNm5TpkxRmzZt1KRJk4v2veOOOzRkyBDdfvvtql69ukqVKqX169c7prN/5k9R29asUNTG7FL3z+rVq2v69Ol6+eWXNWnSJPn7+6ty5cr6+uuvJUm+vr6qXr361Si50HjyySf1119/qWXLlnr++ecVEBCgjz/+WGfOnNHrr7+u6dOna+nSperUqZOnSy1Q8jtuXl5eHq74yrEZY4yniyjs8nMQIkn333+/pk+f7nQQUqxYsStcZcHCmAEF186dO9W+fXvdcccdGjZsmOrXr39J833zzTfKyspSu3bt5O3tzRsfwBXg7v556NAhlSxZUsWLF5ckPfHEE3rvvfe0YsUK3XDDDVey5AIhP8cdhw4d0ssvv6w5c+Y4Aun8+fPl6+urVq1aacCAAYqPj78KVXse45Z/hKvLxEFI/jFmQMG3adMm3XfffWratKmGDx+uBg0a5Gt+9k/gysnP/mmMkc1mU1ZWlry8vPTTTz9p6tSpmjNnjhYvXqxmzZpdxco9g0DqHsbNPYQrC3AQkn+MGVDwbdq0Sffee6+aN29+0f3UfuBmZz+gA3Bl5Gf/zL4/njhxQkuXLlXjxo2LxIGuHYHUPYxb/hGuLMJBSP4xZkDBdyn7afb9cerUqapTp446dux4tUsFihx39s8GDRqoXbt2V7vUAoFA6h7GLX+u3W+TXWXNmjXTm2++qZSUFCUlJennn3922c8Y4wgJU6dO1YoVK4psSGDMgILvYvtp9j+k06dP14MPPqgjR454olSgyHFn/zx48KAnSi0Q8nPckT2Qbtq0ST169ChSASE7xi2frtp1CYuI7HeM37Jli9O07De4nTZtWo4bbBZVjBlQ8Nn30yFDhjhuBpn9UrpTp041pUuXNp999pmnSgSKLPbP/OG4wz2M26UhXF0BvMjlH2MGFHwpKSkmLCzM9O7d2/z222+O9jfeeMOULl26yP4hBQoC9s/84bjDPYzbxRGurhBe5PKPMQMKvnXr1pm4uDjHH9OkpCQTGBhoPv30Uw9XBoD9M3847nAP45Y3wtUVxItc/jFmQMFn//hHamqquf32282HH37o4YoA2LF/5g/HHe5h3HLH1QKvMPP/X+5LS0tTXFyc7r77bvXt29fTZRVojBlQOBhjdOzYMZUpU8bTpQD4B/bPS8dxh3sYN9cIV1cJL3L5x5gBAICrheMO9zBuzghXAAAAAGAB7nMFAAAAABYgXAEAAACABQhXAAAAAGABwhUAAAAAWIBwBQAAAAAWIFwBAAqNQYMGyWaz5Xj89ttvl73smTNncilhAMBl8fF0AQAA5EenTp309ttvO7VVqFDBQ9W4du7cORUrVszTZQAArjLOXAEAChU/Pz8FBwc7Pby9vfX555+refPm8vf3V40aNTRu3DidP3/eMd/EiRPVqFEjlShRQiEhIXrwwQd14sQJSdLKlSsVFxenY8eOOc6GPf3005Ikm82mefPmOdVQpkwZzZw5U5K0a9cu2Ww2zZ49WxEREfL399cHH3wgSXrzzTdVr149+fv7q27dunr99dcdy8jIyFB8fLwqVaokf39/VatWTYmJiVdu4AAAVxxnrgAAhd7q1as1YMAAvfrqq2rXrp1+//133XfffZKksWPHSpK8vLz06quvqnr16vrjjz/04IMP6rHHHtPrr7+uNm3aKCkpSWPGjNH27dslSSVLlsxXDaNGjdKECRPUrFkzR8AaM2aMJk2apGbNmmnTpk0aMmSISpQooYEDB+rVV1/V/Pnz9fHHH+v666/X3r17tXfvXmsHBgBwVRGuAACFysKFC52Cz+23366///5bo0aN0sCBAyVJNWrU0DPPPKPHHnvMEa6GDx/umCc0NFTPPvusHnjgAb3++uvy9fVVYGCgbDabgoOD3apr+PDh6tmzp+PnsWPHasKECY626tWra+vWrXrjjTc0cOBA7dmzR7Vq1dJNN90km82matWqufW8AICCg3AFAChUOnbsqClTpjh+LlGihBo3bqzvvvtOzz33nKM9MzNTZ86c0alTpxQQEKCvv/5aiYmJ+uWXX5Senq7z5887Tb9cLVu2dPz/5MmT+v333zV48GANGTLE0X7+/HkFBgZKunBxjltvvVV16tRRp06d1KVLF912222XXQcAwHMIVwCAQqVEiRKqWbOmU9uJEyc0btw4pzNHdv7+/tq1a5e6dOmif/3rX3ruued03XXX6dtvv9XgwYOVkZGRZ7iy2Wwyxji1nTt3zmVd2euRpOnTpys8PNypn7e3tySpefPm2rlzp7788kt9/fXX6tOnjyIjIzVnzpyLjAAAoKAiXAEACr3mzZtr+/btOUKX3caNG5WVlaUJEybIy+vCtZw+/vhjpz6+vr7KzMzMMW+FChV04MABx8+//vqrTp06lWc9QUFBqly5sv744w/1798/136lS5dWTEyMYmJi1Lt3b3Xq1ElHjhzRddddl+fyAQAFE+EKAFDojRkzRl26dNH111+v3r17y8vLSz/88IO2bNmiZ599VjVr1tS5c+f02muv6c4779R3332nqVOnOi0jNDRUJ06c0LJly9SkSRMFBAQoICBAN998syZNmqTWrVsrMzNTI0eOvKTLrI8bN05Dhw5VYGCgOnXqpLNnz2rDhg36+++/lZCQoIkTJ6pSpUpq1qyZvLy89Mknnyg4OJh7bQFAIcal2AEAhV5UVJQWLlyor776SmFhYbrxxhv18ssvOy4S0aRJE02cOFHjx49Xw4YN9cEHH+S47HmbNm30wAMPKCYmRhUqVNCLL74oSZowYYJCQkLUrl079evXT//+978v6Tta9957r9588029/fbbatSokSIiIjRz5kxVr15dklSqVCm9+OKLatmypcLCwrRr1y4tWrTIcWYNAFD42Mw/P0gOAAAAAMg33h4DAAAAAAsQrgAAAADAAoQrAAAAALAA4QoAAAAALEC4AgAAAAALEK4AAAAAwAKEKwAAAACwAOEKAAAAACxAuAIAAAAACxCuAAAAAMAChCsAAAAAsADhCgAAAAAs8H9razItFJnphwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': rf_model_v1.feature_names_in_,\n",
    "    'Importance': rf_model_v1.feature_importances_\n",
    "})\n",
    "\n",
    "feature_importance_df = feature_importance_df[feature_importance_df['Importance'] > 0]\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=feature_importance_df['Feature'], y=feature_importance_df['Importance'])\n",
    "\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.title(\"Feature Importances from Random Forest Model\", fontsize=15)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ce166cc3-aec6-448b-afb1-3de4eef41941",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"rf_model_v1.pkl\", \"wb\") as file:\n",
    "    pickle.dump(rf_model_v1, file)\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "05b9f4c0-aebe-46fb-84ce-46c60cf4ac97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(random_state=42)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"rf_model_v1.pkl\", \"rb\") as file:\n",
    "    model_v1 = pickle.load(file)\n",
    "\n",
    "print(model_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7d4b95-6334-43d8-881c-956b1eab96f1",
   "metadata": {},
   "source": [
    "### Model Implementation for Edge Devices (Pending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4dcac9e4-8f6a-4a48-b389-4daaaa93fec5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\danish\\anaconda3\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.12.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.69.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in c:\\users\\danish\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\danish\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\danish\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0e945c70-f307-48a3-9483-7429744c6235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8da57ef6-bdc8-4c99-ad2e-ab80b3b73b8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    SAMPLES = 1000\n",
    "    np.random.seed(1337)\n",
    "    x_values = np.random.uniform(low=0, high=2*math.pi, size=SAMPLES)\n",
    "\n",
    "    np.random.shuffle(x_values)\n",
    "    y_values = np.sin(x_values)\n",
    "    y_values += 0.1 * np.random.randn(*y_values.shape)\n",
    "\n",
    "    TRAIN_SPLIT =  int(0.6 * SAMPLES)\n",
    "    TEST_SPLIT = int(0.2 * SAMPLES + TRAIN_SPLIT)\n",
    "    x_train, x_test, x_validate = np.split(x_values, [TRAIN_SPLIT, TEST_SPLIT])\n",
    "    y_train, y_test, y_validate = np.split(y_values, [TRAIN_SPLIT, TEST_SPLIT])\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(16, activation='relu', input_shape=(1,)))\n",
    "    model.add(layers.Dense(16, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    model.fit(x_train, y_train, epochs=100, batch_size=16,\n",
    "                        validation_data=(x_validate, y_validate))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7bcb73e6-6200-44a3-99a8-33b1224301d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Danish\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.4047 - mae: 0.5495 - val_loss: 0.3856 - val_mae: 0.5332\n",
      "Epoch 2/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2990 - mae: 0.4763 - val_loss: 0.3117 - val_mae: 0.4875\n",
      "Epoch 3/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2379 - mae: 0.4206 - val_loss: 0.2593 - val_mae: 0.4479\n",
      "Epoch 4/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2179 - mae: 0.4106 - val_loss: 0.2447 - val_mae: 0.4378\n",
      "Epoch 5/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2001 - mae: 0.3917 - val_loss: 0.2061 - val_mae: 0.4017\n",
      "Epoch 6/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1781 - mae: 0.3679 - val_loss: 0.1796 - val_mae: 0.3719\n",
      "Epoch 7/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1645 - mae: 0.3471 - val_loss: 0.1664 - val_mae: 0.3541\n",
      "Epoch 8/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1566 - mae: 0.3380 - val_loss: 0.1689 - val_mae: 0.3561\n",
      "Epoch 9/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1697 - mae: 0.3545 - val_loss: 0.1511 - val_mae: 0.3350\n",
      "Epoch 10/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1494 - mae: 0.3289 - val_loss: 0.1442 - val_mae: 0.3230\n",
      "Epoch 11/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1444 - mae: 0.3210 - val_loss: 0.1499 - val_mae: 0.3193\n",
      "Epoch 12/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1321 - mae: 0.2981 - val_loss: 0.1361 - val_mae: 0.3107\n",
      "Epoch 13/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1400 - mae: 0.3026 - val_loss: 0.1325 - val_mae: 0.3045\n",
      "Epoch 14/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1288 - mae: 0.2919 - val_loss: 0.1299 - val_mae: 0.3001\n",
      "Epoch 15/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1320 - mae: 0.2928 - val_loss: 0.1767 - val_mae: 0.3325\n",
      "Epoch 16/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1291 - mae: 0.2849 - val_loss: 0.1255 - val_mae: 0.2858\n",
      "Epoch 17/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1289 - mae: 0.2884 - val_loss: 0.1225 - val_mae: 0.2768\n",
      "Epoch 18/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1136 - mae: 0.2585 - val_loss: 0.1350 - val_mae: 0.2909\n",
      "Epoch 19/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1154 - mae: 0.2645 - val_loss: 0.1137 - val_mae: 0.2705\n",
      "Epoch 20/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1295 - mae: 0.2790 - val_loss: 0.1155 - val_mae: 0.2694\n",
      "Epoch 21/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1236 - mae: 0.2731 - val_loss: 0.1137 - val_mae: 0.2652\n",
      "Epoch 22/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1090 - mae: 0.2608 - val_loss: 0.1081 - val_mae: 0.2568\n",
      "Epoch 23/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1113 - mae: 0.2596 - val_loss: 0.1051 - val_mae: 0.2532\n",
      "Epoch 24/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1040 - mae: 0.2421 - val_loss: 0.1007 - val_mae: 0.2463\n",
      "Epoch 25/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1021 - mae: 0.2413 - val_loss: 0.1000 - val_mae: 0.2449\n",
      "Epoch 26/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1126 - mae: 0.2549 - val_loss: 0.1036 - val_mae: 0.2462\n",
      "Epoch 27/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1091 - mae: 0.2524 - val_loss: 0.0959 - val_mae: 0.2340\n",
      "Epoch 28/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1021 - mae: 0.2383 - val_loss: 0.0940 - val_mae: 0.2328\n",
      "Epoch 29/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1052 - mae: 0.2456 - val_loss: 0.0898 - val_mae: 0.2254\n",
      "Epoch 30/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1054 - mae: 0.2428 - val_loss: 0.0866 - val_mae: 0.2211\n",
      "Epoch 31/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1037 - mae: 0.2383 - val_loss: 0.1132 - val_mae: 0.2547\n",
      "Epoch 32/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0900 - mae: 0.2246 - val_loss: 0.1053 - val_mae: 0.2438\n",
      "Epoch 33/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0876 - mae: 0.2229 - val_loss: 0.0793 - val_mae: 0.2086\n",
      "Epoch 34/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0886 - mae: 0.2191 - val_loss: 0.0793 - val_mae: 0.2089\n",
      "Epoch 35/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0744 - mae: 0.2051 - val_loss: 0.0813 - val_mae: 0.2126\n",
      "Epoch 36/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0819 - mae: 0.2102 - val_loss: 0.0701 - val_mae: 0.1922\n",
      "Epoch 37/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0828 - mae: 0.2116 - val_loss: 0.0681 - val_mae: 0.1909\n",
      "Epoch 38/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0803 - mae: 0.2109 - val_loss: 0.0658 - val_mae: 0.1794\n",
      "Epoch 39/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0725 - mae: 0.1955 - val_loss: 0.0645 - val_mae: 0.1858\n",
      "Epoch 40/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0677 - mae: 0.1894 - val_loss: 0.0669 - val_mae: 0.1934\n",
      "Epoch 41/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0758 - mae: 0.2055 - val_loss: 0.0585 - val_mae: 0.1756\n",
      "Epoch 42/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0649 - mae: 0.1824 - val_loss: 0.0581 - val_mae: 0.1786\n",
      "Epoch 43/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0560 - mae: 0.1749 - val_loss: 0.0597 - val_mae: 0.1813\n",
      "Epoch 44/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0644 - mae: 0.1903 - val_loss: 0.0596 - val_mae: 0.1850\n",
      "Epoch 45/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0575 - mae: 0.1738 - val_loss: 0.0503 - val_mae: 0.1561\n",
      "Epoch 46/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0566 - mae: 0.1742 - val_loss: 0.0527 - val_mae: 0.1726\n",
      "Epoch 47/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0562 - mae: 0.1769 - val_loss: 0.0589 - val_mae: 0.1818\n",
      "Epoch 48/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0587 - mae: 0.1782 - val_loss: 0.0459 - val_mae: 0.1582\n",
      "Epoch 49/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0497 - mae: 0.1620 - val_loss: 0.0429 - val_mae: 0.1519\n",
      "Epoch 50/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0595 - mae: 0.1791 - val_loss: 0.0405 - val_mae: 0.1471\n",
      "Epoch 51/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0455 - mae: 0.1573 - val_loss: 0.0402 - val_mae: 0.1501\n",
      "Epoch 52/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0421 - mae: 0.1527 - val_loss: 0.0402 - val_mae: 0.1526\n",
      "Epoch 53/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0427 - mae: 0.1513 - val_loss: 0.0352 - val_mae: 0.1343\n",
      "Epoch 54/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0466 - mae: 0.1604 - val_loss: 0.0348 - val_mae: 0.1399\n",
      "Epoch 55/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0429 - mae: 0.1546 - val_loss: 0.0384 - val_mae: 0.1509\n",
      "Epoch 56/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0386 - mae: 0.1451 - val_loss: 0.0406 - val_mae: 0.1535\n",
      "Epoch 57/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0347 - mae: 0.1365 - val_loss: 0.0364 - val_mae: 0.1460\n",
      "Epoch 58/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0374 - mae: 0.1445 - val_loss: 0.0294 - val_mae: 0.1296\n",
      "Epoch 59/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0364 - mae: 0.1421 - val_loss: 0.0282 - val_mae: 0.1270\n",
      "Epoch 60/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0321 - mae: 0.1354 - val_loss: 0.0349 - val_mae: 0.1451\n",
      "Epoch 61/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0326 - mae: 0.1374 - val_loss: 0.0300 - val_mae: 0.1186\n",
      "Epoch 62/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0340 - mae: 0.1381 - val_loss: 0.0263 - val_mae: 0.1167\n",
      "Epoch 63/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0299 - mae: 0.1303 - val_loss: 0.0234 - val_mae: 0.1170\n",
      "Epoch 64/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0309 - mae: 0.1327 - val_loss: 0.0226 - val_mae: 0.1152\n",
      "Epoch 65/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0260 - mae: 0.1234 - val_loss: 0.0222 - val_mae: 0.1043\n",
      "Epoch 66/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0287 - mae: 0.1288 - val_loss: 0.0223 - val_mae: 0.1179\n",
      "Epoch 67/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0289 - mae: 0.1299 - val_loss: 0.0245 - val_mae: 0.1054\n",
      "Epoch 68/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0276 - mae: 0.1273 - val_loss: 0.0196 - val_mae: 0.1049\n",
      "Epoch 69/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0269 - mae: 0.1225 - val_loss: 0.0183 - val_mae: 0.1010\n",
      "Epoch 70/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0253 - mae: 0.1216 - val_loss: 0.0230 - val_mae: 0.1193\n",
      "Epoch 71/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0235 - mae: 0.1199 - val_loss: 0.0224 - val_mae: 0.1185\n",
      "Epoch 72/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0246 - mae: 0.1204 - val_loss: 0.0205 - val_mae: 0.1035\n",
      "Epoch 73/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0226 - mae: 0.1121 - val_loss: 0.0180 - val_mae: 0.0979\n",
      "Epoch 74/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0204 - mae: 0.1104 - val_loss: 0.0160 - val_mae: 0.0981\n",
      "Epoch 75/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0228 - mae: 0.1185 - val_loss: 0.0287 - val_mae: 0.1302\n",
      "Epoch 76/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0208 - mae: 0.1078 - val_loss: 0.0161 - val_mae: 0.0944\n",
      "Epoch 77/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0187 - mae: 0.1079 - val_loss: 0.0159 - val_mae: 0.0924\n",
      "Epoch 78/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0209 - mae: 0.1100 - val_loss: 0.0183 - val_mae: 0.1094\n",
      "Epoch 79/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0219 - mae: 0.1153 - val_loss: 0.0141 - val_mae: 0.0921\n",
      "Epoch 80/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0190 - mae: 0.1061 - val_loss: 0.0183 - val_mae: 0.1024\n",
      "Epoch 81/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0186 - mae: 0.1038 - val_loss: 0.0148 - val_mae: 0.0922\n",
      "Epoch 82/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0189 - mae: 0.1065 - val_loss: 0.0165 - val_mae: 0.0990\n",
      "Epoch 83/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0195 - mae: 0.1102 - val_loss: 0.0177 - val_mae: 0.1036\n",
      "Epoch 84/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0185 - mae: 0.1036 - val_loss: 0.0157 - val_mae: 0.1003\n",
      "Epoch 85/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0191 - mae: 0.1064 - val_loss: 0.0130 - val_mae: 0.0844\n",
      "Epoch 86/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0173 - mae: 0.1043 - val_loss: 0.0185 - val_mae: 0.1102\n",
      "Epoch 87/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0168 - mae: 0.1028 - val_loss: 0.0153 - val_mae: 0.0959\n",
      "Epoch 88/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0185 - mae: 0.1065 - val_loss: 0.0165 - val_mae: 0.0939\n",
      "Epoch 89/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0168 - mae: 0.1012 - val_loss: 0.0266 - val_mae: 0.1211\n",
      "Epoch 90/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0174 - mae: 0.1040 - val_loss: 0.0278 - val_mae: 0.1275\n",
      "Epoch 91/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0169 - mae: 0.1019 - val_loss: 0.0238 - val_mae: 0.1212\n",
      "Epoch 92/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0201 - mae: 0.1093 - val_loss: 0.0172 - val_mae: 0.0993\n",
      "Epoch 93/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0169 - mae: 0.1044 - val_loss: 0.0151 - val_mae: 0.0991\n",
      "Epoch 94/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0182 - mae: 0.1076 - val_loss: 0.0171 - val_mae: 0.1018\n",
      "Epoch 95/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0173 - mae: 0.1026 - val_loss: 0.0114 - val_mae: 0.0846\n",
      "Epoch 96/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0160 - mae: 0.0989 - val_loss: 0.0129 - val_mae: 0.0845\n",
      "Epoch 97/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0152 - mae: 0.0965 - val_loss: 0.0157 - val_mae: 0.1000\n",
      "Epoch 98/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0149 - mae: 0.0961 - val_loss: 0.0190 - val_mae: 0.1026\n",
      "Epoch 99/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0164 - mae: 0.1024 - val_loss: 0.0105 - val_mae: 0.0815\n",
      "Epoch 100/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0142 - mae: 0.0938 - val_loss: 0.0161 - val_mae: 0.0966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Danish\\AppData\\Local\\Temp\\tmpz71ir2ae\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Danish\\AppData\\Local\\Temp\\tmpz71ir2ae\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\Danish\\AppData\\Local\\Temp\\tmpz71ir2ae'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 1), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1664739968016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1664739968784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1664739969168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1664739969360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1664739969552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1664739968592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n",
      "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3168"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model()\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open(\"sine_model_quantized.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "58cac88c-60a7-40d8-a04c-61cd1a62c8f7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tinymlgen in c:\\users\\danish\\anaconda3\\lib\\site-packages (0.2)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tinymlgen) (2.18.0)\n",
      "Requirement already satisfied: hexdump in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tinymlgen) (3.3)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow->tinymlgen) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->tinymlgen) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->tinymlgen) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->tinymlgen) (24.12.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->tinymlgen) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->tinymlgen) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->tinymlgen) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->tinymlgen) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->tinymlgen) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->tinymlgen) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->tinymlgen) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->tinymlgen) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->tinymlgen) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->tinymlgen) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->tinymlgen) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->tinymlgen) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->tinymlgen) (1.69.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->tinymlgen) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->tinymlgen) (3.7.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->tinymlgen) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->tinymlgen) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->tinymlgen) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->tinymlgen) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow->tinymlgen) (0.41.2)\n",
      "Requirement already satisfied: rich in c:\\users\\danish\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow->tinymlgen) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\danish\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow->tinymlgen) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\danish\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow->tinymlgen) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow->tinymlgen) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow->tinymlgen) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow->tinymlgen) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow->tinymlgen) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow->tinymlgen) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow->tinymlgen) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow->tinymlgen) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow->tinymlgen) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow->tinymlgen) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow->tinymlgen) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\danish\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow->tinymlgen) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tinymlgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3be24092-87d6-48c8-9b3f-f6f975ee4938",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Danish\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.7359 - mae: 0.7569 - val_loss: 0.4957 - val_mae: 0.6111\n",
      "Epoch 2/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3942 - mae: 0.5403 - val_loss: 0.4334 - val_mae: 0.5662\n",
      "Epoch 3/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.3606 - mae: 0.5172 - val_loss: 0.4010 - val_mae: 0.5646\n",
      "Epoch 4/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3124 - mae: 0.4860 - val_loss: 0.3303 - val_mae: 0.4968\n",
      "Epoch 5/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2580 - mae: 0.4405 - val_loss: 0.2912 - val_mae: 0.4759\n",
      "Epoch 6/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2441 - mae: 0.4292 - val_loss: 0.2705 - val_mae: 0.4635\n",
      "Epoch 7/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2181 - mae: 0.4112 - val_loss: 0.2253 - val_mae: 0.4155\n",
      "Epoch 8/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2116 - mae: 0.4057 - val_loss: 0.2049 - val_mae: 0.3968\n",
      "Epoch 9/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1811 - mae: 0.3741 - val_loss: 0.1911 - val_mae: 0.3805\n",
      "Epoch 10/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1733 - mae: 0.3608 - val_loss: 0.1879 - val_mae: 0.3789\n",
      "Epoch 11/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1805 - mae: 0.3665 - val_loss: 0.1778 - val_mae: 0.3620\n",
      "Epoch 12/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1677 - mae: 0.3466 - val_loss: 0.1868 - val_mae: 0.3584\n",
      "Epoch 13/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1713 - mae: 0.3430 - val_loss: 0.1673 - val_mae: 0.3527\n",
      "Epoch 14/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1711 - mae: 0.3462 - val_loss: 0.1592 - val_mae: 0.3376\n",
      "Epoch 15/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1589 - mae: 0.3312 - val_loss: 0.1551 - val_mae: 0.3335\n",
      "Epoch 16/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1512 - mae: 0.3198 - val_loss: 0.1521 - val_mae: 0.3274\n",
      "Epoch 17/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1563 - mae: 0.3239 - val_loss: 0.1680 - val_mae: 0.3371\n",
      "Epoch 18/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1620 - mae: 0.3273 - val_loss: 0.1422 - val_mae: 0.3107\n",
      "Epoch 19/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1525 - mae: 0.3146 - val_loss: 0.1402 - val_mae: 0.3073\n",
      "Epoch 20/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1690 - mae: 0.3292 - val_loss: 0.1447 - val_mae: 0.3078\n",
      "Epoch 21/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1565 - mae: 0.3143 - val_loss: 0.1376 - val_mae: 0.2962\n",
      "Epoch 22/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1293 - mae: 0.2851 - val_loss: 0.1471 - val_mae: 0.3063\n",
      "Epoch 23/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1337 - mae: 0.2844 - val_loss: 0.1330 - val_mae: 0.2916\n",
      "Epoch 24/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1421 - mae: 0.2922 - val_loss: 0.1276 - val_mae: 0.2817\n",
      "Epoch 25/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1496 - mae: 0.3004 - val_loss: 0.1298 - val_mae: 0.2839\n",
      "Epoch 26/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1319 - mae: 0.2809 - val_loss: 0.1242 - val_mae: 0.2770\n",
      "Epoch 27/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1351 - mae: 0.2829 - val_loss: 0.1250 - val_mae: 0.2782\n",
      "Epoch 28/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.1310 - mae: 0.2793 - val_loss: 0.1229 - val_mae: 0.2750\n",
      "Epoch 29/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1250 - mae: 0.2679 - val_loss: 0.1206 - val_mae: 0.2713\n",
      "Epoch 30/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1336 - mae: 0.2822 - val_loss: 0.1269 - val_mae: 0.2773\n",
      "Epoch 31/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.1194 - mae: 0.2663 - val_loss: 0.1163 - val_mae: 0.2646\n",
      "Epoch 32/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1254 - mae: 0.2672 - val_loss: 0.1171 - val_mae: 0.2674\n",
      "Epoch 33/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1222 - mae: 0.2674 - val_loss: 0.1128 - val_mae: 0.2579\n",
      "Epoch 34/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1195 - mae: 0.2624 - val_loss: 0.1117 - val_mae: 0.2580\n",
      "Epoch 35/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1152 - mae: 0.2567 - val_loss: 0.1104 - val_mae: 0.2558\n",
      "Epoch 36/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1151 - mae: 0.2562 - val_loss: 0.1093 - val_mae: 0.2491\n",
      "Epoch 37/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1204 - mae: 0.2656 - val_loss: 0.1084 - val_mae: 0.2554\n",
      "Epoch 38/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1162 - mae: 0.2539 - val_loss: 0.1044 - val_mae: 0.2474\n",
      "Epoch 39/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1085 - mae: 0.2499 - val_loss: 0.1081 - val_mae: 0.2519\n",
      "Epoch 40/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1001 - mae: 0.2395 - val_loss: 0.1132 - val_mae: 0.2540\n",
      "Epoch 41/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1206 - mae: 0.2640 - val_loss: 0.1083 - val_mae: 0.2491\n",
      "Epoch 42/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1106 - mae: 0.2512 - val_loss: 0.1001 - val_mae: 0.2399\n",
      "Epoch 43/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1029 - mae: 0.2399 - val_loss: 0.1025 - val_mae: 0.2427\n",
      "Epoch 44/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1110 - mae: 0.2534 - val_loss: 0.1026 - val_mae: 0.2412\n",
      "Epoch 45/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0970 - mae: 0.2310 - val_loss: 0.0949 - val_mae: 0.2303\n",
      "Epoch 46/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1013 - mae: 0.2420 - val_loss: 0.0916 - val_mae: 0.2270\n",
      "Epoch 47/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0967 - mae: 0.2347 - val_loss: 0.1041 - val_mae: 0.2431\n",
      "Epoch 48/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0934 - mae: 0.2329 - val_loss: 0.0891 - val_mae: 0.2231\n",
      "Epoch 49/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0976 - mae: 0.2339 - val_loss: 0.0851 - val_mae: 0.2167\n",
      "Epoch 50/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0990 - mae: 0.2362 - val_loss: 0.0850 - val_mae: 0.2153\n",
      "Epoch 51/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0882 - mae: 0.2205 - val_loss: 0.1065 - val_mae: 0.2441\n",
      "Epoch 52/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0912 - mae: 0.2277 - val_loss: 0.0791 - val_mae: 0.2066\n",
      "Epoch 53/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0760 - mae: 0.2006 - val_loss: 0.0779 - val_mae: 0.2069\n",
      "Epoch 54/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0987 - mae: 0.2325 - val_loss: 0.0761 - val_mae: 0.1991\n",
      "Epoch 55/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0768 - mae: 0.1950 - val_loss: 0.0737 - val_mae: 0.2004\n",
      "Epoch 56/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0915 - mae: 0.2236 - val_loss: 0.0711 - val_mae: 0.1880\n",
      "Epoch 57/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0784 - mae: 0.2041 - val_loss: 0.0693 - val_mae: 0.1920\n",
      "Epoch 58/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0808 - mae: 0.2097 - val_loss: 0.0685 - val_mae: 0.1910\n",
      "Epoch 59/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0777 - mae: 0.2042 - val_loss: 0.0723 - val_mae: 0.2010\n",
      "Epoch 60/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0662 - mae: 0.1919 - val_loss: 0.0648 - val_mae: 0.1871\n",
      "Epoch 61/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0673 - mae: 0.1855 - val_loss: 0.0661 - val_mae: 0.1908\n",
      "Epoch 62/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0656 - mae: 0.1885 - val_loss: 0.0630 - val_mae: 0.1875\n",
      "Epoch 63/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0668 - mae: 0.1872 - val_loss: 0.0585 - val_mae: 0.1763\n",
      "Epoch 64/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0636 - mae: 0.1852 - val_loss: 0.0635 - val_mae: 0.1883\n",
      "Epoch 65/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0640 - mae: 0.1873 - val_loss: 0.0580 - val_mae: 0.1766\n",
      "Epoch 66/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0603 - mae: 0.1786 - val_loss: 0.0570 - val_mae: 0.1775\n",
      "Epoch 67/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0613 - mae: 0.1831 - val_loss: 0.0497 - val_mae: 0.1628\n",
      "Epoch 68/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0512 - mae: 0.1631 - val_loss: 0.0575 - val_mae: 0.1797\n",
      "Epoch 69/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0528 - mae: 0.1732 - val_loss: 0.0469 - val_mae: 0.1593\n",
      "Epoch 70/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0511 - mae: 0.1695 - val_loss: 0.0434 - val_mae: 0.1506\n",
      "Epoch 71/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0511 - mae: 0.1660 - val_loss: 0.0494 - val_mae: 0.1625\n",
      "Epoch 72/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0493 - mae: 0.1577 - val_loss: 0.0441 - val_mae: 0.1608\n",
      "Epoch 73/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0493 - mae: 0.1679 - val_loss: 0.0408 - val_mae: 0.1449\n",
      "Epoch 74/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0471 - mae: 0.1596 - val_loss: 0.0541 - val_mae: 0.1743\n",
      "Epoch 75/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0454 - mae: 0.1573 - val_loss: 0.0495 - val_mae: 0.1704\n",
      "Epoch 76/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0397 - mae: 0.1494 - val_loss: 0.0386 - val_mae: 0.1501\n",
      "Epoch 77/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0448 - mae: 0.1571 - val_loss: 0.0340 - val_mae: 0.1396\n",
      "Epoch 78/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0461 - mae: 0.1620 - val_loss: 0.0427 - val_mae: 0.1589\n",
      "Epoch 79/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0338 - mae: 0.1355 - val_loss: 0.0397 - val_mae: 0.1439\n",
      "Epoch 80/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0383 - mae: 0.1396 - val_loss: 0.0300 - val_mae: 0.1312\n",
      "Epoch 81/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0362 - mae: 0.1444 - val_loss: 0.0289 - val_mae: 0.1189\n",
      "Epoch 82/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0325 - mae: 0.1359 - val_loss: 0.0279 - val_mae: 0.1293\n",
      "Epoch 83/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0294 - mae: 0.1293 - val_loss: 0.0349 - val_mae: 0.1469\n",
      "Epoch 84/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0344 - mae: 0.1385 - val_loss: 0.0248 - val_mae: 0.1219\n",
      "Epoch 85/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0268 - mae: 0.1249 - val_loss: 0.0222 - val_mae: 0.1112\n",
      "Epoch 86/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0269 - mae: 0.1269 - val_loss: 0.0249 - val_mae: 0.1178\n",
      "Epoch 87/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0272 - mae: 0.1248 - val_loss: 0.0260 - val_mae: 0.1161\n",
      "Epoch 88/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0288 - mae: 0.1264 - val_loss: 0.0266 - val_mae: 0.1289\n",
      "Epoch 89/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0209 - mae: 0.1106 - val_loss: 0.0338 - val_mae: 0.1437\n",
      "Epoch 90/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0266 - mae: 0.1231 - val_loss: 0.0219 - val_mae: 0.1176\n",
      "Epoch 91/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0260 - mae: 0.1244 - val_loss: 0.0201 - val_mae: 0.1135\n",
      "Epoch 92/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0247 - mae: 0.1184 - val_loss: 0.0174 - val_mae: 0.1005\n",
      "Epoch 93/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0200 - mae: 0.1069 - val_loss: 0.0188 - val_mae: 0.0995\n",
      "Epoch 94/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0200 - mae: 0.1073 - val_loss: 0.0193 - val_mae: 0.1111\n",
      "Epoch 95/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0204 - mae: 0.1106 - val_loss: 0.0233 - val_mae: 0.1051\n",
      "Epoch 96/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0205 - mae: 0.1097 - val_loss: 0.0157 - val_mae: 0.0990\n",
      "Epoch 97/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0200 - mae: 0.1066 - val_loss: 0.0165 - val_mae: 0.0935\n",
      "Epoch 98/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0194 - mae: 0.1075 - val_loss: 0.0284 - val_mae: 0.1306\n",
      "Epoch 99/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0214 - mae: 0.1137 - val_loss: 0.0157 - val_mae: 0.1004\n",
      "Epoch 100/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0195 - mae: 0.1090 - val_loss: 0.0156 - val_mae: 0.0923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Danish\\AppData\\Local\\Temp\\tmpqgw064t_\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Danish\\AppData\\Local\\Temp\\tmpqgw064t_\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\Danish\\AppData\\Local\\Temp\\tmpqgw064t_'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 1), dtype=tf.float32, name='keras_tensor_7')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1664733842128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1664733841936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1664733847696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1664733845392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1664733849616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1664733846928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n",
      "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#ifdef __has_attribute\n",
      "#define HAVE_ATTRIBUTE(x) __has_attribute(x)\n",
      "#else\n",
      "#define HAVE_ATTRIBUTE(x) 0\n",
      "#endif\n",
      "#if HAVE_ATTRIBUTE(aligned) || (defined(__GNUC__) && !defined(__clang__))\n",
      "#define DATA_ALIGN_ATTRIBUTE __attribute__((aligned(4)))\n",
      "#else\n",
      "#define DATA_ALIGN_ATTRIBUTE\n",
      "#endif\n",
      "\n",
      "const unsigned char model_data[] DATA_ALIGN_ATTRIBUTE = {\n",
      "\t0x1c, 0x00, 0x00, 0x00, 0x54, 0x46, 0x4c, 0x33, 0x14, 0x00, 0x20, 0x00, \n",
      "\t0x1c, 0x00, 0x18, 0x00, 0x14, 0x00, 0x10, 0x00, 0x0c, 0x00, 0x00, 0x00, \n",
      "\t0x08, 0x00, 0x04, 0x00, 0x14, 0x00, 0x00, 0x00, 0x1c, 0x00, 0x00, 0x00, \n",
      "\t0x98, 0x00, 0x00, 0x00, 0xf0, 0x00, 0x00, 0x00, 0x14, 0x07, 0x00, 0x00, \n",
      "\t0x24, 0x07, 0x00, 0x00, 0x08, 0x0c, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, \n",
      "\t0x01, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0a, 0x00, \n",
      "\t0x10, 0x00, 0x0c, 0x00, 0x08, 0x00, 0x04, 0x00, 0x0a, 0x00, 0x00, 0x00, \n",
      "\t0x0c, 0x00, 0x00, 0x00, 0x1c, 0x00, 0x00, 0x00, 0x3c, 0x00, 0x00, 0x00, \n",
      "\t0x0f, 0x00, 0x00, 0x00, 0x73, 0x65, 0x72, 0x76, 0x69, 0x6e, 0x67, 0x5f, \n",
      "\t0x64, 0x65, 0x66, 0x61, 0x75, 0x6c, 0x74, 0x00, 0x01, 0x00, 0x00, 0x00, \n",
      "\t0x04, 0x00, 0x00, 0x00, 0x90, 0xff, 0xff, 0xff, 0x09, 0x00, 0x00, 0x00, \n",
      "\t0x04, 0x00, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, 0x6f, 0x75, 0x74, 0x70, \n",
      "\t0x75, 0x74, 0x5f, 0x30, 0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, \n",
      "\t0x04, 0x00, 0x00, 0x00, 0xba, 0xf9, 0xff, 0xff, 0x04, 0x00, 0x00, 0x00, \n",
      "\t0x0e, 0x00, 0x00, 0x00, 0x6b, 0x65, 0x72, 0x61, 0x73, 0x5f, 0x74, 0x65, \n",
      "\t0x6e, 0x73, 0x6f, 0x72, 0x5f, 0x37, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, \n",
      "\t0x34, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0xdc, 0xff, 0xff, 0xff, \n",
      "\t0x0c, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x13, 0x00, 0x00, 0x00, \n",
      "\t0x43, 0x4f, 0x4e, 0x56, 0x45, 0x52, 0x53, 0x49, 0x4f, 0x4e, 0x5f, 0x4d, \n",
      "\t0x45, 0x54, 0x41, 0x44, 0x41, 0x54, 0x41, 0x00, 0x08, 0x00, 0x0c, 0x00, \n",
      "\t0x08, 0x00, 0x04, 0x00, 0x08, 0x00, 0x00, 0x00, 0x0b, 0x00, 0x00, 0x00, \n",
      "\t0x04, 0x00, 0x00, 0x00, 0x13, 0x00, 0x00, 0x00, 0x6d, 0x69, 0x6e, 0x5f, \n",
      "\t0x72, 0x75, 0x6e, 0x74, 0x69, 0x6d, 0x65, 0x5f, 0x76, 0x65, 0x72, 0x73, \n",
      "\t0x69, 0x6f, 0x6e, 0x00, 0x0d, 0x00, 0x00, 0x00, 0x20, 0x06, 0x00, 0x00, \n",
      "\t0x18, 0x06, 0x00, 0x00, 0xc8, 0x05, 0x00, 0x00, 0xb0, 0x01, 0x00, 0x00, \n",
      "\t0x9c, 0x01, 0x00, 0x00, 0x4c, 0x01, 0x00, 0x00, 0xfc, 0x00, 0x00, 0x00, \n",
      "\t0xac, 0x00, 0x00, 0x00, 0xa4, 0x00, 0x00, 0x00, 0x9c, 0x00, 0x00, 0x00, \n",
      "\t0x94, 0x00, 0x00, 0x00, 0x74, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, \n",
      "\t0x6a, 0xfa, 0xff, 0xff, 0x04, 0x00, 0x00, 0x00, 0x60, 0x00, 0x00, 0x00, \n",
      "\t0x0c, 0x00, 0x00, 0x00, 0x08, 0x00, 0x0e, 0x00, 0x08, 0x00, 0x04, 0x00, \n",
      "\t0x08, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0x28, 0x00, 0x00, 0x00, \n",
      "\t0x00, 0x00, 0x06, 0x00, 0x08, 0x00, 0x04, 0x00, 0x06, 0x00, 0x00, 0x00, \n",
      "\t0x04, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0xea, 0x03, 0x00, 0x00, \n",
      "\t0x0c, 0x00, 0x18, 0x00, 0x14, 0x00, 0x10, 0x00, 0x0c, 0x00, 0x04, 0x00, \n",
      "\t0x0c, 0x00, 0x00, 0x00, 0xec, 0xb7, 0xa8, 0x9a, 0xd0, 0xf4, 0x0a, 0xe3, \n",
      "\t0x02, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, \n",
      "\t0x06, 0x00, 0x00, 0x00, 0x32, 0x2e, 0x31, 0x38, 0x2e, 0x30, 0x00, 0x00, \n",
      "\t0xd6, 0xfa, 0xff, 0xff, 0x04, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, \n",
      "\t0x31, 0x2e, 0x35, 0x2e, 0x30, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, \n",
      "\t0x00, 0x00, 0x00, 0x00, 0xd8, 0xf5, 0xff, 0xff, 0xdc, 0xf5, 0xff, 0xff, \n",
      "\t0xe0, 0xf5, 0xff, 0xff, 0xfe, 0xfa, 0xff, 0xff, 0x04, 0x00, 0x00, 0x00, \n",
      "\t0x40, 0x00, 0x00, 0x00, 0x71, 0x63, 0x01, 0x3f, 0x98, 0x71, 0xfd, 0xbd, \n",
      "\t0x45, 0x13, 0x58, 0xbd, 0x2f, 0x29, 0xa1, 0xbe, 0xdb, 0x6b, 0x03, 0xbf, \n",
      "\t0x28, 0x23, 0x1c, 0x3f, 0x01, 0xd7, 0x4f, 0x3e, 0x90, 0x39, 0x44, 0xbe, \n",
      "\t0x4f, 0xcd, 0x0f, 0xbf, 0x4d, 0x00, 0x09, 0x3f, 0xa2, 0x26, 0xc3, 0xbe, \n",
      "\t0x0b, 0xe0, 0x9e, 0x3e, 0xb8, 0x4d, 0x70, 0xbe, 0x15, 0xc5, 0xb9, 0x3e, \n",
      "\t0xb9, 0xb3, 0xc8, 0x3e, 0x5a, 0x32, 0xcc, 0xbe, 0x4a, 0xfb, 0xff, 0xff, \n",
      "\t0x04, 0x00, 0x00, 0x00, 0x40, 0x00, 0x00, 0x00, 0xf8, 0xd1, 0xc7, 0x3e, \n",
      "\t0x00, 0x00, 0x00, 0x00, 0xc4, 0x7a, 0x08, 0x3f, 0x00, 0x00, 0x00, 0x00, \n",
      "\t0x00, 0x00, 0x00, 0x00, 0x62, 0x2a, 0x44, 0xbe, 0xb9, 0x08, 0x2f, 0xbe, \n",
      "\t0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xf9, 0xd5, 0xea, 0xbe, \n",
      "\t0x00, 0x00, 0x00, 0x00, 0x03, 0xed, 0xb6, 0x3e, 0x00, 0x00, 0x00, 0x00, \n",
      "\t0xa5, 0x4c, 0x01, 0x3e, 0xa5, 0xd3, 0x74, 0xbf, 0x00, 0x00, 0x00, 0x00, \n",
      "\t0x96, 0xfb, 0xff, 0xff, 0x04, 0x00, 0x00, 0x00, 0x40, 0x00, 0x00, 0x00, \n",
      "\t0x7b, 0x11, 0xce, 0x3d, 0x0c, 0xe8, 0x06, 0xbe, 0x61, 0x51, 0x6f, 0xbb, \n",
      "\t0xcc, 0x3e, 0xb9, 0xbd, 0x00, 0x00, 0x00, 0x00, 0x91, 0xf2, 0x0b, 0x3e, \n",
      "\t0xb3, 0xb1, 0x05, 0x3d, 0x96, 0x2c, 0x84, 0xbe, 0x91, 0xc7, 0x41, 0x3f, \n",
      "\t0xe9, 0xac, 0x15, 0x3c, 0x10, 0x8b, 0xea, 0xbd, 0x4d, 0x1d, 0x09, 0x3e, \n",
      "\t0x01, 0x2d, 0x9f, 0x3d, 0x86, 0x14, 0xc6, 0x3e, 0xde, 0x1f, 0x0a, 0x3f, \n",
      "\t0x00, 0x00, 0x00, 0x00, 0xe2, 0xfb, 0xff, 0xff, 0x04, 0x00, 0x00, 0x00, \n",
      "\t0x04, 0x00, 0x00, 0x00, 0xed, 0x4c, 0x0e, 0xbe, 0xf2, 0xfb, 0xff, 0xff, \n",
      "\t0x04, 0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0xe0, 0x83, 0x40, 0xbe, \n",
      "\t0x5b, 0x2d, 0xce, 0x3e, 0x23, 0x2e, 0xca, 0x3d, 0xef, 0xf0, 0x8d, 0x3e, \n",
      "\t0x42, 0xb4, 0x2a, 0x3e, 0x58, 0x39, 0xde, 0x3e, 0x5b, 0x90, 0xc1, 0x3e, \n",
      "\t0xd2, 0xee, 0x5c, 0x3e, 0x2a, 0x9a, 0x48, 0x3e, 0x0d, 0xcd, 0xf4, 0xbd, \n",
      "\t0x62, 0xe0, 0x39, 0x3e, 0x50, 0xa1, 0xc1, 0x3e, 0x90, 0x5a, 0x0b, 0xbe, \n",
      "\t0x00, 0xda, 0x5f, 0x3c, 0x0b, 0x48, 0x97, 0xbe, 0x3b, 0x44, 0xa4, 0xbe, \n",
      "\t0x3d, 0xb2, 0x29, 0x3e, 0xb9, 0xa1, 0x41, 0xbe, 0xd3, 0xf3, 0x03, 0xbf, \n",
      "\t0x34, 0x42, 0x53, 0xbe, 0x3a, 0x64, 0x98, 0xbe, 0x9b, 0xe5, 0x93, 0x3e, \n",
      "\t0xf7, 0xb5, 0x46, 0xbe, 0x83, 0x7d, 0xb6, 0x3e, 0x66, 0x18, 0x10, 0x3e, \n",
      "\t0xa0, 0x1c, 0x9d, 0xbd, 0xd2, 0xca, 0x26, 0x3e, 0xdf, 0x9a, 0x70, 0xbd, \n",
      "\t0xbe, 0xc4, 0x51, 0x3e, 0x8d, 0xe6, 0x96, 0x3e, 0x4f, 0x4b, 0xc2, 0x3e, \n",
      "\t0x72, 0xfe, 0x90, 0xbe, 0xae, 0x2e, 0x85, 0xbe, 0xb1, 0x90, 0xa6, 0x3e, \n",
      "\t0xc6, 0x90, 0x79, 0x3e, 0x20, 0xad, 0xae, 0xbe, 0x34, 0x24, 0x88, 0xbe, \n",
      "\t0x28, 0x12, 0x56, 0x3e, 0x98, 0x7f, 0x8c, 0x3e, 0xc0, 0x41, 0x1e, 0xbe, \n",
      "\t0xc8, 0x3a, 0x81, 0xbe, 0x0d, 0x42, 0x95, 0x3d, 0xd2, 0x59, 0x0c, 0x3e, \n",
      "\t0x99, 0xa5, 0xc6, 0xbe, 0x22, 0x4d, 0xda, 0xbe, 0x7b, 0xc0, 0xce, 0xbe, \n",
      "\t0x61, 0x6d, 0xd7, 0xbb, 0xcf, 0xaa, 0xa8, 0x3e, 0x27, 0xa2, 0x88, 0xbd, \n",
      "\t0xc5, 0x00, 0x87, 0x3e, 0x3d, 0x70, 0x34, 0x3e, 0x1d, 0x37, 0x87, 0x3e, \n",
      "\t0x82, 0x79, 0x90, 0xbe, 0x98, 0x6a, 0x92, 0xbe, 0x64, 0xe8, 0xcc, 0x3d, \n",
      "\t0x90, 0x27, 0xd1, 0x3c, 0xd8, 0x96, 0xb1, 0xbe, 0x2a, 0x84, 0xcd, 0xbe, \n",
      "\t0xa7, 0x09, 0xb1, 0xbe, 0x64, 0x6b, 0x5f, 0x3e, 0x16, 0xc0, 0x0a, 0x3e, \n",
      "\t0x06, 0x81, 0xdc, 0xbe, 0xa0, 0x7e, 0x7c, 0x3d, 0xa0, 0x8d, 0x63, 0xbc, \n",
      "\t0xb2, 0x23, 0x44, 0x3e, 0x58, 0xc3, 0x33, 0xbe, 0xa3, 0xf2, 0xa6, 0xbe, \n",
      "\t0xc6, 0x77, 0x24, 0x3e, 0xbb, 0x86, 0x73, 0xbe, 0xbc, 0x5c, 0x97, 0xbe, \n",
      "\t0xf8, 0x55, 0x62, 0x3d, 0x2e, 0x65, 0x1e, 0xbe, 0x30, 0xe1, 0x91, 0xbc, \n",
      "\t0x47, 0x35, 0x9c, 0xbe, 0xd9, 0xb6, 0x88, 0x3e, 0xb2, 0xc7, 0xaf, 0xbe, \n",
      "\t0x00, 0x74, 0xf8, 0x3a, 0x30, 0xf6, 0x5b, 0xbe, 0xde, 0x6b, 0x1d, 0x3e, \n",
      "\t0x33, 0x60, 0x82, 0x3e, 0xaf, 0x56, 0x77, 0x3e, 0x98, 0xf3, 0x84, 0xbe, \n",
      "\t0x78, 0xc6, 0xb5, 0x3e, 0x35, 0xa0, 0x85, 0x3e, 0xae, 0x03, 0xba, 0xbe, \n",
      "\t0x32, 0xa1, 0x95, 0x3e, 0xe7, 0x19, 0xaa, 0xbd, 0xf4, 0x0b, 0x88, 0xbe, \n",
      "\t0x7f, 0xa2, 0xbc, 0x3e, 0xf5, 0x15, 0x88, 0xbe, 0xcd, 0xe3, 0xc6, 0x3e, \n",
      "\t0x34, 0xa1, 0xad, 0x3e, 0x80, 0x76, 0xc8, 0x3c, 0xa5, 0x12, 0xef, 0xbe, \n",
      "\t0x25, 0xce, 0xeb, 0xbe, 0x00, 0xc9, 0x26, 0xbe, 0x3d, 0xbf, 0xb1, 0xbd, \n",
      "\t0x9b, 0x85, 0xb4, 0xbe, 0x1d, 0x66, 0xd6, 0x3b, 0xd3, 0x33, 0xb8, 0x3e, \n",
      "\t0x92, 0x8d, 0x0c, 0x3e, 0x53, 0xfb, 0x82, 0xbe, 0x21, 0x29, 0x80, 0x3e, \n",
      "\t0x8b, 0x14, 0xb6, 0x3e, 0x26, 0x28, 0x18, 0x3e, 0xf5, 0x3d, 0x93, 0xbe, \n",
      "\t0x06, 0x2a, 0x27, 0x3e, 0xb3, 0xcd, 0xb7, 0x3e, 0x70, 0x40, 0x3a, 0x3d, \n",
      "\t0x74, 0x97, 0xd0, 0xbe, 0xb0, 0xc7, 0x9e, 0xbe, 0x8c, 0x1f, 0xd8, 0xbd, \n",
      "\t0x39, 0x93, 0x96, 0xbe, 0xc2, 0x11, 0x1c, 0x3e, 0x2d, 0x83, 0x4d, 0xbf, \n",
      "\t0x8b, 0x50, 0xb3, 0xbe, 0x26, 0x37, 0x2a, 0x3e, 0xec, 0x83, 0x3d, 0x3e, \n",
      "\t0x13, 0x21, 0xd4, 0x3d, 0x6a, 0xf2, 0x49, 0x3e, 0x38, 0x6a, 0xd1, 0xbe, \n",
      "\t0xdc, 0x8d, 0x1c, 0x3e, 0x80, 0xc4, 0xe2, 0xbc, 0xa1, 0xfa, 0x50, 0xbe, \n",
      "\t0x0e, 0x4a, 0x3c, 0x3e, 0xc7, 0xfd, 0x68, 0x3e, 0x70, 0x06, 0xd6, 0x3e, \n",
      "\t0xe0, 0xcb, 0x92, 0xbd, 0xbb, 0xbc, 0xb9, 0x3e, 0xf8, 0x3b, 0x10, 0xbe, \n",
      "\t0xdb, 0x6e, 0xc5, 0x3e, 0xb0, 0xed, 0x83, 0xbd, 0x8e, 0x07, 0x63, 0x3e, \n",
      "\t0x80, 0x06, 0x99, 0xbe, 0xae, 0x13, 0x81, 0xbe, 0xb8, 0xe8, 0x1c, 0xbe, \n",
      "\t0x4b, 0x70, 0xd1, 0x3e, 0x39, 0x9d, 0xfc, 0xbe, 0xe0, 0x8a, 0x2f, 0xbc, \n",
      "\t0x69, 0x6b, 0xb2, 0x3d, 0x1f, 0x64, 0xdc, 0x3e, 0xbe, 0x26, 0x89, 0x3e, \n",
      "\t0x67, 0x80, 0x57, 0xbf, 0x72, 0x6c, 0x13, 0x3e, 0x0b, 0x65, 0x9f, 0x3e, \n",
      "\t0x9a, 0x39, 0x7a, 0x3e, 0x82, 0x66, 0x4e, 0x3f, 0x40, 0x47, 0x48, 0xbc, \n",
      "\t0xc7, 0x94, 0x96, 0x3e, 0x5a, 0x53, 0xb3, 0x3e, 0xd1, 0xd2, 0x3e, 0x3d, \n",
      "\t0x70, 0x41, 0x6a, 0xbd, 0x79, 0x81, 0xa9, 0x3e, 0xcb, 0x70, 0x52, 0x3e, \n",
      "\t0x5e, 0x36, 0xa8, 0xbe, 0x06, 0x6b, 0x7a, 0x3e, 0x75, 0xb1, 0x8a, 0xbe, \n",
      "\t0x7f, 0x43, 0x4a, 0xbe, 0x9b, 0x18, 0x57, 0x3d, 0xb2, 0xfe, 0x1f, 0xbe, \n",
      "\t0x7a, 0x64, 0x92, 0x3e, 0xf3, 0xe1, 0xb8, 0x3e, 0x77, 0xea, 0x12, 0xbf, \n",
      "\t0x78, 0xe1, 0xa0, 0xbd, 0xb3, 0x90, 0x82, 0x3e, 0x1f, 0x7f, 0x20, 0xbe, \n",
      "\t0xec, 0x2d, 0xa5, 0x3e, 0x80, 0x72, 0x5d, 0xbc, 0xcc, 0x99, 0xad, 0xbe, \n",
      "\t0x83, 0x57, 0x78, 0xbe, 0x7b, 0x26, 0xc4, 0x3e, 0x25, 0xaa, 0xa7, 0x3e, \n",
      "\t0xeb, 0x18, 0x65, 0xbe, 0xde, 0x84, 0xbe, 0x3e, 0x19, 0x15, 0x15, 0xbd, \n",
      "\t0x58, 0xbc, 0x0f, 0xbd, 0x4a, 0x2c, 0xb4, 0x3e, 0x7f, 0xbc, 0xb3, 0x3e, \n",
      "\t0x17, 0x4c, 0x08, 0x3e, 0xda, 0xb1, 0x27, 0xbe, 0xd4, 0x3c, 0xe9, 0xbd, \n",
      "\t0x24, 0xc4, 0xfe, 0xbe, 0x2b, 0xc2, 0x8a, 0x3e, 0xa8, 0x60, 0x9b, 0xbe, \n",
      "\t0x96, 0x75, 0x47, 0x3e, 0xdb, 0x3e, 0xe1, 0x3d, 0x58, 0xe1, 0x79, 0xbe, \n",
      "\t0x89, 0x4b, 0x33, 0x3e, 0xb6, 0xeb, 0x8a, 0xbe, 0x08, 0x93, 0x1a, 0x3e, \n",
      "\t0x59, 0xd1, 0xdb, 0xbe, 0x64, 0x1f, 0xf3, 0x3d, 0xe7, 0x79, 0x49, 0xbd, \n",
      "\t0xeb, 0xab, 0xcf, 0x3e, 0x46, 0x2d, 0xda, 0x3d, 0x9e, 0xe7, 0x5f, 0x3e, \n",
      "\t0xfc, 0xd5, 0xbc, 0xbe, 0xfd, 0x06, 0x4b, 0x3e, 0xe1, 0x92, 0xa5, 0xbd, \n",
      "\t0x65, 0x54, 0x99, 0x3e, 0x97, 0x7e, 0x8e, 0x3e, 0xf1, 0x9d, 0x55, 0x3e, \n",
      "\t0x9a, 0x09, 0x17, 0x3e, 0x4c, 0xeb, 0xac, 0xbe, 0x16, 0xed, 0xdc, 0xbe, \n",
      "\t0xff, 0x9e, 0x93, 0x3e, 0x2f, 0x28, 0x2d, 0xbe, 0x40, 0x2e, 0x34, 0x3c, \n",
      "\t0xbc, 0xef, 0x87, 0xbe, 0xef, 0x8f, 0xbf, 0x3e, 0x17, 0x68, 0x09, 0x3f, \n",
      "\t0x9f, 0x5c, 0xb2, 0x3e, 0xea, 0xd3, 0x12, 0x3e, 0x74, 0x42, 0xc8, 0xbe, \n",
      "\t0xd2, 0x54, 0xeb, 0xbd, 0x24, 0x2c, 0xc9, 0xbd, 0x50, 0x03, 0xa6, 0x3c, \n",
      "\t0x06, 0x01, 0xdf, 0xbe, 0x28, 0x3c, 0x65, 0xbd, 0xa9, 0x9d, 0xd8, 0xbe, \n",
      "\t0xee, 0x4e, 0xbd, 0xbe, 0x04, 0xeb, 0x6a, 0x3e, 0x8d, 0x35, 0xd5, 0xbe, \n",
      "\t0x5c, 0x3b, 0xb5, 0x3d, 0xd9, 0x3d, 0x06, 0xbe, 0x72, 0x5d, 0x68, 0xbe, \n",
      "\t0x99, 0xa8, 0x61, 0x3f, 0xc0, 0x45, 0x52, 0xbd, 0xcc, 0x2c, 0xf1, 0xbd, \n",
      "\t0x98, 0xee, 0x54, 0xbf, 0x69, 0x22, 0x79, 0x3e, 0x6c, 0x84, 0xcc, 0x3d, \n",
      "\t0x00, 0x8a, 0x1e, 0xba, 0x58, 0x7a, 0xf6, 0xbe, 0xb0, 0x64, 0x5c, 0xbe, \n",
      "\t0x7c, 0xdc, 0x14, 0xbd, 0x90, 0xa9, 0xc6, 0x3c, 0x0b, 0xec, 0x3d, 0x3e, \n",
      "\t0x3a, 0x0d, 0x23, 0xbe, 0xd4, 0x57, 0xa3, 0x3d, 0x3f, 0x51, 0xd2, 0xbe, \n",
      "\t0x2b, 0xbf, 0x82, 0x3e, 0x56, 0xb4, 0x17, 0x3e, 0x23, 0xe1, 0xc7, 0xbe, \n",
      "\t0x84, 0x49, 0xb1, 0xbd, 0x5e, 0xb7, 0x1b, 0x3e, 0x3e, 0xcb, 0xba, 0xbe, \n",
      "\t0xa4, 0x03, 0xc8, 0xbd, 0xb4, 0x68, 0xbd, 0x3d, 0xf8, 0xa5, 0x68, 0xbd, \n",
      "\t0x7b, 0xc0, 0xc1, 0x3e, 0xb4, 0x18, 0xcb, 0xbd, 0x34, 0x49, 0x95, 0x3d, \n",
      "\t0x98, 0xb7, 0x6b, 0xbe, 0xc0, 0x87, 0x05, 0x3d, 0xd8, 0x2a, 0xd4, 0xbd, \n",
      "\t0x00, 0x00, 0x06, 0x00, 0x08, 0x00, 0x04, 0x00, 0x06, 0x00, 0x00, 0x00, \n",
      "\t0x04, 0x00, 0x00, 0x00, 0x40, 0x00, 0x00, 0x00, 0x5e, 0x2a, 0xea, 0xbe, \n",
      "\t0x75, 0xe1, 0xe6, 0x3e, 0x9a, 0x52, 0x5a, 0xbe, 0xfd, 0x5f, 0x3a, 0x3c, \n",
      "\t0xe8, 0x4b, 0x04, 0x3f, 0x19, 0x55, 0x66, 0xbe, 0x9a, 0x16, 0x93, 0xbe, \n",
      "\t0xb3, 0x99, 0x88, 0x3f, 0xcf, 0xfe, 0xda, 0x3f, 0x04, 0x7e, 0x92, 0xbe, \n",
      "\t0x9b, 0xab, 0x47, 0x3e, 0x66, 0x51, 0x1b, 0xbe, 0x90, 0xe3, 0xa4, 0xbe, \n",
      "\t0xb5, 0x20, 0x3d, 0xbf, 0x4e, 0x60, 0x7c, 0xbf, 0xd4, 0x86, 0x21, 0xbe, \n",
      "\t0x38, 0xfb, 0xff, 0xff, 0x3c, 0xfb, 0xff, 0xff, 0x0f, 0x00, 0x00, 0x00, \n",
      "\t0x4d, 0x4c, 0x49, 0x52, 0x20, 0x43, 0x6f, 0x6e, 0x76, 0x65, 0x72, 0x74, \n",
      "\t0x65, 0x64, 0x2e, 0x00, 0x01, 0x00, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00, \n",
      "\t0x00, 0x00, 0x0e, 0x00, 0x18, 0x00, 0x14, 0x00, 0x10, 0x00, 0x0c, 0x00, \n",
      "\t0x08, 0x00, 0x04, 0x00, 0x0e, 0x00, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00, \n",
      "\t0x1c, 0x00, 0x00, 0x00, 0xd8, 0x00, 0x00, 0x00, 0xdc, 0x00, 0x00, 0x00, \n",
      "\t0xe0, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x6d, 0x61, 0x69, 0x6e, \n",
      "\t0x00, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 0x80, 0x00, 0x00, 0x00, \n",
      "\t0x38, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x9a, 0xff, 0xff, 0xff, \n",
      "\t0x10, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x08, 0x0c, 0x00, 0x00, 0x00, \n",
      "\t0x10, 0x00, 0x00, 0x00, 0xb4, 0xfb, 0xff, 0xff, 0x01, 0x00, 0x00, 0x00, \n",
      "\t0x09, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, \n",
      "\t0x01, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 0xca, 0xff, 0xff, 0xff, \n",
      "\t0x10, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x08, 0x10, 0x00, 0x00, 0x00, \n",
      "\t0x14, 0x00, 0x00, 0x00, 0xba, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x01, \n",
      "\t0x01, 0x00, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, \n",
      "\t0x07, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, \n",
      "\t0x00, 0x00, 0x0e, 0x00, 0x16, 0x00, 0x00, 0x00, 0x10, 0x00, 0x0c, 0x00, \n",
      "\t0x0b, 0x00, 0x04, 0x00, 0x0e, 0x00, 0x00, 0x00, 0x18, 0x00, 0x00, 0x00, \n",
      "\t0x00, 0x00, 0x00, 0x08, 0x18, 0x00, 0x00, 0x00, 0x1c, 0x00, 0x00, 0x00, \n",
      "\t0x00, 0x00, 0x06, 0x00, 0x08, 0x00, 0x07, 0x00, 0x06, 0x00, 0x00, 0x00, \n",
      "\t0x00, 0x00, 0x00, 0x01, 0x01, 0x00, 0x00, 0x00, 0x07, 0x00, 0x00, 0x00, \n",
      "\t0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x06, 0x00, 0x00, 0x00, \n",
      "\t0x05, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x09, 0x00, 0x00, 0x00, \n",
      "\t0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0a, 0x00, 0x00, 0x00, \n",
      "\t0x74, 0x03, 0x00, 0x00, 0x1c, 0x03, 0x00, 0x00, 0xc4, 0x02, 0x00, 0x00, \n",
      "\t0x88, 0x02, 0x00, 0x00, 0x4c, 0x02, 0x00, 0x00, 0xf0, 0x01, 0x00, 0x00, \n",
      "\t0xa0, 0x01, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x60, 0x00, 0x00, 0x00, \n",
      "\t0x04, 0x00, 0x00, 0x00, 0xca, 0xfc, 0xff, 0xff, 0x00, 0x00, 0x00, 0x01, \n",
      "\t0x14, 0x00, 0x00, 0x00, 0x1c, 0x00, 0x00, 0x00, 0x1c, 0x00, 0x00, 0x00, \n",
      "\t0x0a, 0x00, 0x00, 0x00, 0x34, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, \n",
      "\t0xff, 0xff, 0xff, 0xff, 0x01, 0x00, 0x00, 0x00, 0xb4, 0xfc, 0xff, 0xff, \n",
      "\t0x1b, 0x00, 0x00, 0x00, 0x53, 0x74, 0x61, 0x74, 0x65, 0x66, 0x75, 0x6c, \n",
      "\t0x50, 0x61, 0x72, 0x74, 0x69, 0x74, 0x69, 0x6f, 0x6e, 0x65, 0x64, 0x43, \n",
      "\t0x61, 0x6c, 0x6c, 0x5f, 0x31, 0x3a, 0x30, 0x00, 0x02, 0x00, 0x00, 0x00, \n",
      "\t0x01, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x22, 0xfd, 0xff, 0xff, \n",
      "\t0x00, 0x00, 0x00, 0x01, 0x14, 0x00, 0x00, 0x00, 0x1c, 0x00, 0x00, 0x00, \n",
      "\t0x1c, 0x00, 0x00, 0x00, 0x09, 0x00, 0x00, 0x00, 0x78, 0x00, 0x00, 0x00, \n",
      "\t0x02, 0x00, 0x00, 0x00, 0xff, 0xff, 0xff, 0xff, 0x10, 0x00, 0x00, 0x00, \n",
      "\t0x0c, 0xfd, 0xff, 0xff, 0x5e, 0x00, 0x00, 0x00, 0x73, 0x65, 0x71, 0x75, \n",
      "\t0x65, 0x6e, 0x74, 0x69, 0x61, 0x6c, 0x5f, 0x31, 0x5f, 0x31, 0x2f, 0x64, \n",
      "\t0x65, 0x6e, 0x73, 0x65, 0x5f, 0x34, 0x5f, 0x31, 0x2f, 0x4d, 0x61, 0x74, \n",
      "\t0x4d, 0x75, 0x6c, 0x3b, 0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x74, 0x69, \n",
      "\t0x61, 0x6c, 0x5f, 0x31, 0x5f, 0x31, 0x2f, 0x64, 0x65, 0x6e, 0x73, 0x65, \n",
      "\t0x5f, 0x34, 0x5f, 0x31, 0x2f, 0x52, 0x65, 0x6c, 0x75, 0x3b, 0x73, 0x65, \n",
      "\t0x71, 0x75, 0x65, 0x6e, 0x74, 0x69, 0x61, 0x6c, 0x5f, 0x31, 0x5f, 0x31, \n",
      "\t0x2f, 0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x34, 0x5f, 0x31, 0x2f, 0x42, \n",
      "\t0x69, 0x61, 0x73, 0x41, 0x64, 0x64, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, \n",
      "\t0x01, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0xbe, 0xfd, 0xff, 0xff, \n",
      "\t0x00, 0x00, 0x00, 0x01, 0x14, 0x00, 0x00, 0x00, 0x1c, 0x00, 0x00, 0x00, \n",
      "\t0x1c, 0x00, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, 0x78, 0x00, 0x00, 0x00, \n",
      "\t0x02, 0x00, 0x00, 0x00, 0xff, 0xff, 0xff, 0xff, 0x10, 0x00, 0x00, 0x00, \n",
      "\t0xa8, 0xfd, 0xff, 0xff, 0x5e, 0x00, 0x00, 0x00, 0x73, 0x65, 0x71, 0x75, \n",
      "\t0x65, 0x6e, 0x74, 0x69, 0x61, 0x6c, 0x5f, 0x31, 0x5f, 0x31, 0x2f, 0x64, \n",
      "\t0x65, 0x6e, 0x73, 0x65, 0x5f, 0x33, 0x5f, 0x31, 0x2f, 0x4d, 0x61, 0x74, \n",
      "\t0x4d, 0x75, 0x6c, 0x3b, 0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x74, 0x69, \n",
      "\t0x61, 0x6c, 0x5f, 0x31, 0x5f, 0x31, 0x2f, 0x64, 0x65, 0x6e, 0x73, 0x65, \n",
      "\t0x5f, 0x33, 0x5f, 0x31, 0x2f, 0x52, 0x65, 0x6c, 0x75, 0x3b, 0x73, 0x65, \n",
      "\t0x71, 0x75, 0x65, 0x6e, 0x74, 0x69, 0x61, 0x6c, 0x5f, 0x31, 0x5f, 0x31, \n",
      "\t0x2f, 0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x33, 0x5f, 0x31, 0x2f, 0x42, \n",
      "\t0x69, 0x61, 0x73, 0x41, 0x64, 0x64, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, \n",
      "\t0x01, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0xae, 0xfe, 0xff, 0xff, \n",
      "\t0x00, 0x00, 0x00, 0x01, 0x10, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, \n",
      "\t0x07, 0x00, 0x00, 0x00, 0x2c, 0x00, 0x00, 0x00, 0x34, 0xfe, 0xff, 0xff, \n",
      "\t0x1f, 0x00, 0x00, 0x00, 0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x74, 0x69, \n",
      "\t0x61, 0x6c, 0x5f, 0x31, 0x5f, 0x31, 0x2f, 0x64, 0x65, 0x6e, 0x73, 0x65, \n",
      "\t0x5f, 0x33, 0x5f, 0x31, 0x2f, 0x4d, 0x61, 0x74, 0x4d, 0x75, 0x6c, 0x00, \n",
      "\t0x02, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, \n",
      "\t0xfa, 0xfe, 0xff, 0xff, 0x00, 0x00, 0x00, 0x01, 0x10, 0x00, 0x00, 0x00, \n",
      "\t0x10, 0x00, 0x00, 0x00, 0x06, 0x00, 0x00, 0x00, 0x3c, 0x00, 0x00, 0x00, \n",
      "\t0x80, 0xfe, 0xff, 0xff, 0x2f, 0x00, 0x00, 0x00, 0x73, 0x65, 0x71, 0x75, \n",
      "\t0x65, 0x6e, 0x74, 0x69, 0x61, 0x6c, 0x5f, 0x31, 0x5f, 0x31, 0x2f, 0x64, \n",
      "\t0x65, 0x6e, 0x73, 0x65, 0x5f, 0x33, 0x5f, 0x31, 0x2f, 0x42, 0x69, 0x61, \n",
      "\t0x73, 0x41, 0x64, 0x64, 0x2f, 0x52, 0x65, 0x61, 0x64, 0x56, 0x61, 0x72, \n",
      "\t0x69, 0x61, 0x62, 0x6c, 0x65, 0x4f, 0x70, 0x00, 0x01, 0x00, 0x00, 0x00, \n",
      "\t0x10, 0x00, 0x00, 0x00, 0x52, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x01, \n",
      "\t0x10, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0x05, 0x00, 0x00, 0x00, \n",
      "\t0x1c, 0x00, 0x00, 0x00, 0xd8, 0xfe, 0xff, 0xff, 0x0f, 0x00, 0x00, 0x00, \n",
      "\t0x61, 0x72, 0x69, 0x74, 0x68, 0x2e, 0x63, 0x6f, 0x6e, 0x73, 0x74, 0x61, \n",
      "\t0x6e, 0x74, 0x33, 0x00, 0x01, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, \n",
      "\t0x8a, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x01, 0x10, 0x00, 0x00, 0x00, \n",
      "\t0x10, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x1c, 0x00, 0x00, 0x00, \n",
      "\t0x10, 0xff, 0xff, 0xff, 0x0f, 0x00, 0x00, 0x00, 0x61, 0x72, 0x69, 0x74, \n",
      "\t0x68, 0x2e, 0x63, 0x6f, 0x6e, 0x73, 0x74, 0x61, 0x6e, 0x74, 0x32, 0x00, \n",
      "\t0x01, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0xc2, 0xff, 0xff, 0xff, \n",
      "\t0x00, 0x00, 0x00, 0x01, 0x10, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, \n",
      "\t0x03, 0x00, 0x00, 0x00, 0x1c, 0x00, 0x00, 0x00, 0x48, 0xff, 0xff, 0xff, \n",
      "\t0x0f, 0x00, 0x00, 0x00, 0x61, 0x72, 0x69, 0x74, 0x68, 0x2e, 0x63, 0x6f, \n",
      "\t0x6e, 0x73, 0x74, 0x61, 0x6e, 0x74, 0x31, 0x00, 0x02, 0x00, 0x00, 0x00, \n",
      "\t0x10, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0x00, 0x00, 0x16, 0x00, \n",
      "\t0x18, 0x00, 0x14, 0x00, 0x00, 0x00, 0x10, 0x00, 0x0c, 0x00, 0x08, 0x00, \n",
      "\t0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x07, 0x00, 0x16, 0x00, 0x00, 0x00, \n",
      "\t0x00, 0x00, 0x00, 0x01, 0x10, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, \n",
      "\t0x02, 0x00, 0x00, 0x00, 0x1c, 0x00, 0x00, 0x00, 0x9c, 0xff, 0xff, 0xff, \n",
      "\t0x0e, 0x00, 0x00, 0x00, 0x61, 0x72, 0x69, 0x74, 0x68, 0x2e, 0x63, 0x6f, \n",
      "\t0x6e, 0x73, 0x74, 0x61, 0x6e, 0x74, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, \n",
      "\t0x01, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0x00, 0x00, 0x16, 0x00, \n",
      "\t0x1c, 0x00, 0x18, 0x00, 0x00, 0x00, 0x14, 0x00, 0x10, 0x00, 0x0c, 0x00, \n",
      "\t0x00, 0x00, 0x00, 0x00, 0x08, 0x00, 0x07, 0x00, 0x16, 0x00, 0x00, 0x00, \n",
      "\t0x00, 0x00, 0x00, 0x01, 0x14, 0x00, 0x00, 0x00, 0x20, 0x00, 0x00, 0x00, \n",
      "\t0x20, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x40, 0x00, 0x00, 0x00, \n",
      "\t0x02, 0x00, 0x00, 0x00, 0xff, 0xff, 0xff, 0xff, 0x01, 0x00, 0x00, 0x00, \n",
      "\t0x04, 0x00, 0x04, 0x00, 0x04, 0x00, 0x00, 0x00, 0x20, 0x00, 0x00, 0x00, \n",
      "\t0x73, 0x65, 0x72, 0x76, 0x69, 0x6e, 0x67, 0x5f, 0x64, 0x65, 0x66, 0x61, \n",
      "\t0x75, 0x6c, 0x74, 0x5f, 0x6b, 0x65, 0x72, 0x61, 0x73, 0x5f, 0x74, 0x65, \n",
      "\t0x6e, 0x73, 0x6f, 0x72, 0x5f, 0x37, 0x3a, 0x30, 0x00, 0x00, 0x00, 0x00, \n",
      "\t0x02, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, \n",
      "\t0x01, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x0c, 0x00, \n",
      "\t0x0b, 0x00, 0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 0x0c, 0x00, 0x00, 0x00, \n",
      "\t0x09, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x09\n",
      "};\n",
      "const int model_data_len = 3164;\n"
     ]
    }
   ],
   "source": [
    "from tinymlgen import port\n",
    "\n",
    "model = get_model()\n",
    "c_code = port(model, pretty_print=True)\n",
    "print(c_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2193b5-2cae-4213-aceb-3c08a0fcb648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
